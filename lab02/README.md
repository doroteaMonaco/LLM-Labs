## Lab 02

The second lab is divided into two parts:
- "T5": we use the T5 encoder-decoder model to generate translations. This part will show how to use HuggingFace tokenizers and models. We will peek into the model's attentions to understand what's going on.
- "Attention": we move away from the field of NLP and focus on the attention mechanism itself. We will predict the next value in a time series. The attention mechanism will be used to learn recurring patterns within the time series.

The following are the files you will use for this lab:
- T5 exercise ([text](./text-01-t5.ipynb)) ([solution](./solution-01-t5.ipynb))
- Attention exercise ([text](./text-02-attention.ipynb)) ([solution](./solution-02-attention.ipynb))