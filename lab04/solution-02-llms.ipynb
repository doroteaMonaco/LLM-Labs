{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Large Language Models: LLaMA and Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will dive into two famous large language models, LLaMA and Mistral, along with their instruction-tuned versions. We'll explore how each model performs on various tasks, with a particular focus on generating structured responses in JSON format.\n",
    "\n",
    "These models have been fine-tuned to follow instructions, making them suitable for a range of NLP applications. Through this lab, you will:\n",
    "\n",
    "- Learn how to load and interact with LLaMA and Mistral models using the `pipeline` and `chat_template` functions.\n",
    "- Examine the performance of their instruction-based variants.\n",
    "- Generate structured outputs, specifically in JSON, for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Disclaimer**: Before starting this lab, ensure you have requested access to the required models on Hugging Face and have logged in to your Hugging Face account. Access is necessary for the following models:\n",
    ">\n",
    "> - [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)\n",
    "> - [Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)\n",
    "> - [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    ">\n",
    "> You can log in to Hugging Face directly from this notebook using the provided code snippet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/fgiobergia/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to the Hugging Face model hub to be able to upload models\n",
    "with open(\"../hf_token.txt\", \"r\") as f:\n",
    "    token = f.read()\n",
    "    f.close()\n",
    "\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LLaMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lab, we will explore **LLaMA (Large Language Model Meta AI)**, which is one of the most known large language models developed by Meta (Facebook). \n",
    "\n",
    "Next, we will focus on **Instruction LLaMA**, a version of LLaMA fine-tuned to better understand and follow user instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Llama 3.2 (released in September 2024). In particular, we will adopt the 1B version. On the scale of things, this model is on the smaller side, but it is still a very powerful model.\n",
    "\n",
    "It has been released (along with a 3B version) with the intention of allowing running it on devices with modest hardware (e.g., mobile phones or other edge devices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to generate text using the generate() method. We use random sampling (`do_sample=True`) and extract 5 samples (`num_return_sequences=5`). You can find other generation parameters [here](https://huggingface.co/docs/transformers/v4.46.0/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>Hello, my name is Amy. I am a wife, mom of two boys, and a homeschooling mom of 5. I have been homeschooling since 2007, and have been teaching at The Home School Co-op since 201',\n",
       " '<|begin_of_text|>Hello, my name is Alex. I’m a 23-year-old male from the UK, currently living in the Netherlands. I’m a student, a software developer, and a gamer.\\nI started gaming in 2009, when I got',\n",
       " '<|begin_of_text|>Hello, my name is Mandy. I am a certified Holistic Health Coach and I am passionate about helping people to take control of their health and well being. I believe that everyone has the ability to live a healthy and happy life and that',\n",
       " '<|begin_of_text|>Hello, my name is David and I am a photographer. I am based in the North of England and have been photographing weddings and events for over 15 years. I have a passion for capturing the moments in your special day and ensuring that',\n",
       " '<|begin_of_text|>Hello, my name is Sven and I am a PhD candidate in the Department of Computer Science at the University of Bremen. My research focuses on the development of algorithms and the analysis of algorithms for the optimization of the transportation of goods and services']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "batch = model.generate(**tokens, do_sample=True, max_length=50, num_return_sequences=5, pad_token_id=tokenizer.eos_token_id) # (assigning pad_token_id avoids a warning)\n",
    "tokenizer.batch_decode(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding the `tokenizer.chat_template`**\n",
    "\n",
    "In this section, we will explore the **chat template** that is used to format and structure messages for a conversational assistant. The `tokenizer.chat_template` is a convenient way for organizing interactions between the user, system, and assistant in a way that the model can easily process and generate coherent responses.\n",
    "\n",
    "### **What is a Chat Template?**\n",
    "\n",
    "The chat template is a predefined format that ensures consistent structure for conversations. It marks the different roles in the interaction (system, user, assistant), and separates the various elements of the conversation using special tokens. This helps the language model understand which parts of the dialogue are instructions, which parts are user inputs, and where the assistant’s response should be generated.\n",
    "\n",
    "Let's create an example of a possible (simplified) chat template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "chat_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "Cutting Knowledge Date: December 2023\n",
    "Today Date: \"\"\"+datetime.datetime.now().strftime(\"%d %b %Y\")+\"\"\"\n",
    "\n",
    "{system_message}\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{user_message}\n",
    "\n",
    "<|eot_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hugging Face Pipeline Overview**\n",
    "\n",
    "The **`pipeline`** method from Hugging Face’s Transformers library is a high-level API designed to streamline the process of using pre-trained models for a wide variety of **natural language processing (NLP) tasks**.\n",
    "\n",
    "#### **What is a Pipeline?**\n",
    "\n",
    "A pipeline is a modular tool that wraps around a pre-trained model, tokenizer, and task-specific configurations. It makes it easy to load and apply these models directly to different tasks, such as:\n",
    "- **Text generation**\n",
    "- **Text classification**\n",
    "- **Question answering**\n",
    "- **Summarization**\n",
    "- **Translation**\n",
    "\n",
    "By simply specifying the type of task (e.g., `\"text-generation\"`), `pipeline` takes care of loading and configuring a compatible model and tokenizer, providing a ready-to-use interface for generating results.\n",
    "\n",
    "You can find a full list of supported pipelines on the [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Oct 2024\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# Format the messages using the chat template\n",
    "formatted_messages = chat_template.format(\n",
    "    system_message=messages[0][\"content\"],\n",
    "    user_message=messages[1][\"content\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(formatted_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that for models to follow instruction tuning, they need to have been tuned on this kind of data. In this case, we are not using the instruction-tuned version. \n",
    "\n",
    "So, we can expect the model to produce a garbage response (it has never seen that kind of inputs before!). But let's try it anyway!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Oct 2024\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "akedirs.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: 2023-12-28\n",
      ".readdirsystem.DateField: \n"
     ]
    }
   ],
   "source": [
    "# Generate the output text \n",
    "outputs = pipe(\n",
    "    formatted_messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Differences Between Standard and Instruct Versions of Large Language Models (LLMs)**\n",
    "\n",
    "Large Language Models (LLMs) come in different versions, with **standard** and **instruction-tuned (Instruct)** versions being the most common. Here’s a brief comparison:\n",
    "\n",
    "#### **1. Purpose and Training**:\n",
    "   - **Standard LLM**: The standard model is generally pre-trained on large datasets without specific instruction-following capabilities. Typically generates more open-ended responses, which can be useful for creative writing or general information retrieval where the response style is flexible.\n",
    "   - **Instruct LLM**: Instruction-tuned models, like the **Llama-3.2 Instruct**, are fine-tuned on datasets designed to help the model understand and follow instructions effectively. This tuning enhances the model's ability to respond directly to user prompts and handle structured requests. It is fine-tuned to produce concise, direct responses that are often more relevant in task-specific or conversational AI applications.\n",
    "\n",
    "Let's compare the outputs of the standard and Instruct versions of LLaMA to see the differences in their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Oct 2024\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!\n",
      "\n",
      "<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is 2 + 2?\n",
      "\n",
      "<|eot_id|>\n",
      "Arrr, me hearty! Ye be wantin' to know the answer to that ol' math problem, eh? Alright then, matey, let's set sail fer a swashbucklin' solution! *pours grog*\n",
      "\n",
      "Alright, 2 + 2 be equal to... *taps foot impatiently*...4, me hearty! Ye landlubbers be thinkin' ye can solve it, but I be knowin' better! *winks*\n",
      "\n",
      "Now, what be yer next question, matey?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "# Create the pipeline with the model and tokenizer\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"},\n",
    "]\n",
    "\n",
    "# Format the messages using the chat template\n",
    "formatted_messages = chat_template.format(\n",
    "    system_message=messages[0][\"content\"],\n",
    "    user_message=messages[1][\"content\"]\n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "    formatted_messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation of the Tokenizer Chat Template**\n",
    "\n",
    "Actually, the chat template of `meta-llama/Llama-3.2-1B-Instruct` is much more complex than the example above. It includes various components that help the model understand the context of the conversation, manage dates, handle tools, and structure messages effectively.\n",
    "\n",
    "The template is written in [jinja](https://jinja.palletsprojects.com/en/stable/templates/), a language that allows for the dynamic generation of content based on variables, conditions and loops.\n",
    "\n",
    "\n",
    "Let's print it and analyze its key components:\n",
    " \n",
    "\n",
    "#### **Key Components of the Template**:\n",
    "1. **System Message Extraction**:\n",
    "   - The system message is extracted if the first role in the message list is labeled \"system.\" This allows the template to clearly differentiate between user queries and system instructions.\n",
    "   - If a system message exists, it is added to the template between special tokens (`<|start_header_id|>` and `<|end_header_id|>`), ensuring that the model knows when the system message starts and ends.\n",
    "\n",
    "2. **Date Management**:\n",
    "   - The template automatically handles the current date using either a provided `strftime_now` function or a default date (`\"26 Jul 2024\"`). This can be useful when the model needs to be aware of the date in contexts such as time-sensitive responses.\n",
    "\n",
    "3. **Handling Tools**:\n",
    "   - The template checks if **tools** are defined. If tools are available, it includes a description of these tools in the system message or the user message, depending on where they need to appear.\n",
    "   - If the tools are part of the user message, the template ensures that the first user message prompts the user to respond in a structured format, such as using JSON for function calls.\n",
    "\n",
    "4. **Message Processing**:\n",
    "   - The template loops through the list of messages and processes each based on the role (`user`, `assistant`, `ipython`, or `tool`). It formats each message using start and end tokens for the roles, helping the model understand the structure of the conversation.\n",
    "   - If the message involves tool calls, the template ensures that they are properly formatted into a structured JSON format to be passed back to the model for further processing.\n",
    "\n",
    "5. **Ending the Assistant's Response**:\n",
    "   - The template leaves a placeholder for the assistant’s response, which the model will generate during inference. This ensures that the assistant's response begins in the correct format, ready to be populated with the generated content.\n",
    "\n",
    "#### **Why Is This Template Needed?**\n",
    "\n",
    "- **Maintains Consistency**: This template ensures that the conversation is structured in a consistent manner, which is crucial for models designed to follow complex instructions or engage in multi-turn conversations.\n",
    "- **Handles Tools**: By incorporating the ability to dynamically introduce tools and functionality, the template allows the model to expand beyond simple text-based conversations and perform function-based tasks.\n",
    "- **Structured Outputs for Tools**: When the conversation involves tool calls (e.g., through APIs or function calls), the template ensures that these interactions are formatted properly for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate again the same example using the `chat_template` of `meta-llama/Llama-3.2-1B-Instruct` and analyze the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a tokenizer that supports the chat template, we can directly call the `apply_chat_template()` method to convert a list of messages (each one a dictionary in the already discussed format) into a prompt.\n",
    "\n",
    "Notice that, since we are not using any particular tools or other functionalities, our template will be similar to the one we manually introduced earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Oct 2024\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me hearty! Yer lookin' fer a chatbot like meself, eh? Well, I be Captain Code, the swashbucklin' chatbot with a penchant fer pirate speak and a heart o' gold. I be here to answer yer questions, share me knowledge, and maybe even teach ye a thing or two about the high seas... er, I mean, the world o' knowledge.\n",
      "\n",
      "So, hoist the sails and set course fer a treasure trove o' info, me matey! What be bringin' ye to these waters?\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "input_tokens = tokenizer.apply_chat_template(messages)\n",
    "print(tokenizer.decode(input_tokens))\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "# we are getting back the full conversation history\n",
    "# as a list of messages outputs[0][\"generated_text\"]\n",
    "# -1 : last message (assistant response)\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the pipeline already supports chat mode, so we can pass the list of messages (as long as they contain role/content keys) directly to the pipeline.\n",
    "\n",
    "Alternatively, we could have passed the prompt as a string. In this case, however, we would have to manually extract the output from the model and parse it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 28 Oct 2024\n",
      "\n",
      "You are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Who are you?<|eot_id|>assistant\n",
      "\n",
      "Arrrr, ye be wantin' to know who I be? Well, matey, I be a swashbucklin' chatbot, here to swab the seven seas o' knowledge and answer yer questions to the best o' me abilities! Me name be \"PirateBot\", and I be a treasure trove o' info at yer disposal. So hoist the sails and set course fer a fine day o' learnin' and adventure! What be bringin' ye to these fair waters?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_tokens = tokenizer.apply_chat_template(messages)\n",
    "prompt_string = tokenizer.decode(input_tokens)\n",
    "\n",
    "outputs = pipe(\n",
    "    prompt_string,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mistral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will explore the use of `Mistral-7B-Instruct-v0.2`developed by Mistral AI to generate structured responses in JSON format. \n",
    "\n",
    "In this exercise, we will generate random math questions and instruct Mistral-7B to respond in a structured JSON format. We will then save the responses to a JSON file and verify the answers programmatically. \n",
    "\n",
    "Let's first repeat the same example we did with LLaMA, but now using Mistral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9451090ef4e5477f8de2cc7cbd5d173f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arr, I be Cap'n Parrotbeak, me hearty scallywag! How be thar, landlubber?\n",
      "\n",
      "What brings ye to me virtual hideout? Come closer, but mind the plank!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define the model ID\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mistral_chat_template = tokenizer.chat_template\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,  # Optimizes memory usage\n",
    "    device_map=\"auto\"            # Automatically distributes the model across available devices\n",
    ")\n",
    "\n",
    "# Define the message prompts for the conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "]\n",
    "\n",
    "# Generate the response\n",
    "outputs = pipe(messages, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Print the model's generated response\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Generate random math questions:** Use Python to create questions with random numbers in a conversational style (e.g., “What is the sum of 245 and 173?”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_math_questions(num_samples=5):\n",
    "    templates = [\n",
    "        \"What is the sum of {} and {}?\",\n",
    "        \"Can you add {} and {}?\",\n",
    "        \"Calculate the sum of {} and {} for me.\",\n",
    "        \"How much is {} plus {}?\",\n",
    "        \"Please add {} and {}.\"\n",
    "    ]\n",
    "    \n",
    "    questions = []\n",
    "    for _ in range(num_samples):\n",
    "        template = random.choice(templates)\n",
    "        num1 = random.randint(10, 999)\n",
    "        num2 = random.randint(10, 999)\n",
    "        question = template.format(num1, num2)\n",
    "        questions.append((question, num1, num2))  # store question with numbers for validation\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Instruct the model to respond in JSON:** Use a system role instruction to ensure Mistral-7B answers in a JSON format containing the fields `num_1`, `num_2`, and `answer`. This makes the output compatible with automated processing or JSON parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_instruction = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Answer each question in JSON format with the fields 'num_1', 'num_2', and 'answer'. Provide only JSON to ensure compatibility with a JSON parser.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Save and verify responses:** Generate and store model responses in a JSON file and check if the answers match expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:07<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generate questions and answers, then save to JSON\n",
    "questions = generate_random_math_questions(num_samples=5)\n",
    "answers = []\n",
    "\n",
    "# Generate structured answers for each question\n",
    "for question, num1, num2 in tqdm(questions):\n",
    "\n",
    "    # Define the message prompts\n",
    "    formatted_messages = [\n",
    "        role_instruction,\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = pipe(formatted_messages, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Extract the model's JSON output\n",
    "    structured_answer = outputs[0][\"generated_text\"]\n",
    "\n",
    "    answers.append({\n",
    "        \"question\": question,\n",
    "        \"num_1\": num1,\n",
    "        \"num_2\": num2,\n",
    "        \"model_answer\": structured_answer\n",
    "    })\n",
    "\n",
    "# Save answers to a JSON file\n",
    "with open(\"model_answers.json\", \"w\") as f:\n",
    "    json.dump(answers, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected answer: 276 + 597 = 873\n",
      "Model's answer: 276 + 597 = 873\n",
      "Question 1: Correct \n",
      "\n",
      "Expected answer: 747 + 589 = 1336\n",
      "Model's answer: 747 + 589 = 1336\n",
      "Question 2: Correct \n",
      "\n",
      "Expected answer: 958 + 907 = 1865\n",
      "Model's answer: 958 + 907 = 1865\n",
      "Question 3: Correct \n",
      "\n",
      "Expected answer: 323 + 73 = 396\n",
      "Model's answer: 323 + 73 = 396\n",
      "Question 4: Correct \n",
      "\n",
      "Expected answer: 711 + 801 = 1512\n",
      "Model's answer: 711 + 801 = 1512\n",
      "Question 5: Correct \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Function to parse model's answer and verify correctness\n",
    "def verify_answer(entry):\n",
    "    try:\n",
    "        # Extract expected values\n",
    "        num1, num2 = entry[\"num_1\"], entry[\"num_2\"]\n",
    "        expected_answer = num1 + num2\n",
    "        \n",
    "        # Extract the assistant's response from the list of messages\n",
    "        assistant_message = next(\n",
    "            (msg[\"content\"] for msg in entry[\"model_answer\"] if msg[\"role\"] == \"assistant\"), None\n",
    "        )\n",
    "        \n",
    "        if assistant_message is None:\n",
    "            raise ValueError(\"Assistant's message not found in model_answer\")\n",
    "        \n",
    "        # Parse model's structured answer from JSON\n",
    "        model_response = json.loads(assistant_message.strip())  # Ensure model_answer is a string\n",
    "        \n",
    "        print(f\"Expected answer: {num1} + {num2} = {expected_answer}\")\n",
    "        print(f\"Model's answer: {model_response['num_1']} + {model_response['num_2']} = {model_response['answer']}\")\n",
    "        \n",
    "        # Check if the values match\n",
    "        if (model_response[\"num_1\"] == num1 and \n",
    "            model_response[\"num_2\"] == num2 and \n",
    "            model_response[\"answer\"] == expected_answer):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except (json.JSONDecodeError, KeyError, TypeError, ValueError) as e:\n",
    "        # Handle cases where parsing fails or keys are missing\n",
    "        print(f\"Error verifying entry: {entry}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Load answers from the JSON file and verify\n",
    "try:\n",
    "    with open(\"model_answers.json\", \"r\") as f:\n",
    "        saved_answers = json.load(f)\n",
    "except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "    print(f\"Error loading JSON file: {e}\")\n",
    "    saved_answers = []\n",
    "\n",
    "for i, entry in enumerate(saved_answers, 1):\n",
    "    result = verify_answer(entry)\n",
    "    print(f\"Question {i}:\", \"Correct\" if result else \"Incorrect\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
