{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: A function to compute average grades\n",
    "\n",
    "In this series of exercises, we will use LLMs to generate code based on specific requirements and then compare the output against predefined test cases to ensure correctness. The focus will be on generating functional code and executing tests to verify that it meets the given specifications.\n",
    "\n",
    "### Step 1: Requirements\n",
    "\n",
    "The function receives the grades of a student on her courses (for simplicity exactly six grades for six courses are considered) and computes the average. Grades can be from 18 to 30, or 30Laude == 33. The average is computed excluding the best and worse grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computed average is: 26.25\n"
     ]
    }
   ],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "#TODO\n",
    "#define here your reference solution\n",
    "\n",
    "def compute_average(grades):\n",
    "    \"\"\"\n",
    "    Computes the average of grades, excluding the best and worst grades.\n",
    "    \n",
    "    Args:\n",
    "        grades (list): A list of six grades, where each grade is between 18 and 33.\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed average.\n",
    "    \"\"\"\n",
    "    if len(grades) != 6:\n",
    "        raise ValueError(\"The list must contain exactly six grades.\")\n",
    "    for grade in grades:\n",
    "        if grade < 18 or grade > 33:\n",
    "            raise ValueError(\"Grades must be between 18 and 33.\")\n",
    "    \n",
    "    sorted_grades = sorted(grades)\n",
    "    sorted_without_extremes = sorted_grades[1:-1]\n",
    "    return sum(sorted_without_extremes) / len(sorted_without_extremes)\n",
    "\n",
    "\n",
    "#TODO\n",
    "#paste here the result of the function as given by chatGPT\n",
    "\n",
    "gpt_result = \"\"\"\n",
    "def compute_average(grades):\n",
    "    \\\"\"\"\n",
    "    Computes the average of grades, excluding the best and worst grades.\n",
    "    \n",
    "    Args:\n",
    "        grades (list): A list of six grades, where each grade is between 18 and 33.\n",
    "    \n",
    "    Returns:\n",
    "        float: The computed average.\n",
    "    \\\"\"\"\n",
    "    if len(grades) != 6:\n",
    "        raise ValueError(\"The list must contain exactly six grades.\")\n",
    "    for grade in grades:\n",
    "        if grade < 18 or grade > 33:\n",
    "            raise ValueError(\"Grades must be between 18 and 33.\")\n",
    "    \n",
    "    sorted_grades = sorted(grades)\n",
    "    sorted_without_extremes = sorted_grades[1:-1]\n",
    "    return sum(sorted_without_extremes) / len(sorted_without_extremes)\"\"\"\n",
    "\n",
    "\n",
    "#the dictionary my_codes will contain all the generated codes. Let's start by adding the one generated by chatgpt\n",
    "\n",
    "my_codes = {}\n",
    "my_codes[\"GPT\"] = gpt_result\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "\n",
    "grades = [18, 25, 30, 33, 22, 28]\n",
    "average = compute_average(grades)\n",
    "print(f\"The computed average is: {average:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test Cases\n",
    "\n",
    "To generate a comprehensive black-box test suite for the compute_average function, we will create tests that cover different equivalence classes, boundary conditions, and typical scenarios.\n",
    "\n",
    "- Valid inputs: \n",
    "  - Standard valid grades within the range of 18 to 33, including \"30Laude\" (33).\n",
    "  - Tests where all grades are different, with some being 30Laude.\n",
    "\n",
    "- Boundary values:\n",
    "  - Input with the lowest valid grade (18) and the highest valid grade (33).\n",
    "\n",
    "- Invalid inputs:\n",
    "  - A grade list that has more or fewer than 6 grades.\n",
    "  - Grades outside the valid range (less than 18 or greater than 33).\n",
    "  - non-numeric inputs\n",
    "\n",
    "- Edge cases:\n",
    "  - All grades are the same.\n",
    "  - All grades are 30Laude (33).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid test cases\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "def test_valid_grades():\n",
    "    grades = [18, 25, 30, 33, 22, 28]\n",
    "    result = compute_average(grades)\n",
    "    assert result == pytest.approx(26.25, rel=1e-2)\n",
    "\n",
    "def test_valid_grades_with_30laude():\n",
    "    grades = [20, 25, 30, 33, 29, 28]\n",
    "    result = compute_average(grades)\n",
    "    assert result == pytest.approx(27.75, rel=1e-2)\n",
    "\n",
    "# Boundary test cases\n",
    "def test_lowest_and_highest_valid_grades():\n",
    "    grades = [18, 19, 20, 30, 33, 25]\n",
    "    result = compute_average(grades)\n",
    "    assert result == pytest.approx(23.5, rel=1e-2)\n",
    "\n",
    "def test_all_30laude():\n",
    "    grades = [33, 33, 33, 33, 33, 33]\n",
    "    result = compute_average(grades)\n",
    "    assert result == 33.0\n",
    "\n",
    "# Invalid test cases\n",
    "def test_more_than_six_grades():\n",
    "    grades = [18, 25, 30, 33, 22, 28, 29]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_less_than_six_grades():\n",
    "    grades = [18, 25, 30, 33, 22]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_invalid_grade_too_low():\n",
    "    grades = [17, 25, 30, 33, 22, 28]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_invalid_grade_too_high():\n",
    "    grades = [18, 25, 30, 34, 22, 28]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_all_grades_are_the_same():\n",
    "    grades = [25, 25, 25, 25, 25, 25]\n",
    "    result = compute_average(grades)\n",
    "    assert result == 25.0\n",
    "\n",
    "# Test cases with the lowest and highest grade removed\n",
    "def test_all_grades_except_highest_and_lowest():\n",
    "    grades = [18, 30, 28, 25, 22, 33]\n",
    "    result = compute_average(grades)\n",
    "    assert result == pytest.approx(26.25, rel=1e-2)\n",
    "\n",
    "# Invalid test cases for non-numeric inputs\n",
    "def test_non_numeric_input_string():\n",
    "    grades = [\"a\", 25, 30, 33, 22, 28]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_non_numeric_input_none():\n",
    "    grades = [None, 25, 30, 33, 22, 28]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_non_numeric_input_boolean():\n",
    "    grades = [True, 25, 30, 33, 22, 28]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)\n",
    "\n",
    "def test_non_numeric_input_mixed():\n",
    "    grades = [18, \"25\", 30, 33, 22, 28]\n",
    "    with pytest.raises(ValueError):\n",
    "        compute_average(grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the test cases\n",
    "\n",
    "This code defines a function run_tests that uses the ipytest library to run test cases within a Jupyter notebook. The ipytest library seeks for test cases in the notebook and launches them. The '-vv' parameter is used to provide a verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m======================================= test session starts =======================================\u001b[0m\n",
      "platform win32 -- Python 3.11.9, pytest-9.0.1, pluggy-1.6.0 -- c:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "cachedir: .pytest_cache\n",
      "rootdir: c:\\Users\\dorot\\OneDrive - Politecnico di Torino\\Desktop\\Magistrale\\Secondo Anno Magistrale\\Large Language Models for Software Engineering\\LLM Labs\\lab09\n",
      "plugins: anyio-4.11.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 14 items\n",
      "\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_valid_grades collected 14 items\n",
      "\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_valid_grades \u001b[32mPASSED\u001b[0m\u001b[32m                              [  7%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_valid_grades_with_30laude \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 14%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_lowest_and_highest_valid_grades \u001b[32mPASSED\u001b[0m\u001b[32m                              [  7%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_valid_grades_with_30laude \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 14%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_lowest_and_highest_valid_grades \u001b[32mPASSED\u001b[0m\u001b[32m           [ 21%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_all_30laude \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 28%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_more_than_six_grades \u001b[32mPASSED\u001b[0m\u001b[32m           [ 21%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_all_30laude \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 28%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_more_than_six_grades \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 35%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_less_than_six_grades \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 42%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_invalid_grade_too_low \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 35%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_less_than_six_grades \u001b[32mPASSED\u001b[0m\u001b[32m                      [ 42%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_invalid_grade_too_low \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 50%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_invalid_grade_too_high \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 57%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_all_grades_are_the_same \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 64%]\u001b[0m\u001b[32mPASSED\u001b[0m\u001b[32m                     [ 50%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_invalid_grade_too_high \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 57%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_all_grades_are_the_same \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 64%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_all_grades_except_highest_and_lowest \u001b[32mPASSED\u001b[0m\u001b[32m      [ 71%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_string \n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_all_grades_except_highest_and_lowest \u001b[32mPASSED\u001b[0m\u001b[32m      [ 71%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_string \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 78%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_none \u001b[31mFAILED\u001b[0m\u001b[31m                  [ 78%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_none \u001b[31mFAILED\u001b[0m\u001b[31m                    [ 85%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_boolean \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 92%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_mixed \u001b[31mFAILED\u001b[0m\u001b[31m                    [ 85%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_boolean \u001b[32mPASSED\u001b[0m\u001b[31m                 [ 92%]\u001b[0m\n",
      "t_fb04578909834ed4801b6f712d55e9b0.py::test_non_numeric_input_mixed \u001b[31mFAILED\u001b[0m\u001b[31m                   [100%]\u001b[0m\u001b[31mFAILED\u001b[0m\u001b[31m                   [100%]\u001b[0m\n",
      "\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_non_numeric_input_string __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_non_numeric_input_string\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grades = [\u001b[33m\"\u001b[39;49;00m\u001b[33ma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m25\u001b[39;49;00m, \u001b[94m30\u001b[39;49;00m, \u001b[94m33\u001b[39;49;00m, \u001b[94m22\u001b[39;49;00m, \u001b[94m28\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           compute_average(grades)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\2611686661.py\u001b[0m:62: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "grades = ['a', 25, 30, 33, 22, 28]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Computes the average of grades, excluding the best and worst grades.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        grades (list): A list of six grades, where each grade is between 18 and 33.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        float: The computed average.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(grades) != \u001b[94m6\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe list must contain exactly six grades.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m grade \u001b[95min\u001b[39;49;00m grades:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mif\u001b[39;49;00m grade < \u001b[94m18\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m grade > \u001b[94m33\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: '<' not supported between instances of 'str' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\1775863479.py\u001b[0m:23: TypeError\n",
      "\u001b[31m\u001b[1m___________________________________ test_non_numeric_input_none ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_non_numeric_input_none\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grades = [\u001b[94mNone\u001b[39;49;00m, \u001b[94m25\u001b[39;49;00m, \u001b[94m30\u001b[39;49;00m, \u001b[94m33\u001b[39;49;00m, \u001b[94m22\u001b[39;49;00m, \u001b[94m28\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           compute_average(grades)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\2611686661.py\u001b[0m:67: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "grades = [None, 25, 30, 33, 22, 28]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Computes the average of grades, excluding the best and worst grades.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        grades (list): A list of six grades, where each grade is between 18 and 33.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        float: The computed average.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(grades) != \u001b[94m6\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe list must contain exactly six grades.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m grade \u001b[95min\u001b[39;49;00m grades:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mif\u001b[39;49;00m grade < \u001b[94m18\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m grade > \u001b[94m33\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: '<' not supported between instances of 'NoneType' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\1775863479.py\u001b[0m:23: TypeError\n",
      "\u001b[31m\u001b[1m__________________________________ test_non_numeric_input_mixed ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_non_numeric_input_mixed\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grades = [\u001b[94m18\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m25\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m30\u001b[39;49;00m, \u001b[94m33\u001b[39;49;00m, \u001b[94m22\u001b[39;49;00m, \u001b[94m28\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           compute_average(grades)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\2611686661.py\u001b[0m:77: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "grades = [18, '25', 30, 33, 22, 28]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Computes the average of grades, excluding the best and worst grades.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        grades (list): A list of six grades, where each grade is between 18 and 33.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        float: The computed average.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(grades) != \u001b[94m6\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe list must contain exactly six grades.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m grade \u001b[95min\u001b[39;49;00m grades:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mif\u001b[39;49;00m grade < \u001b[94m18\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m grade > \u001b[94m33\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: '<' not supported between instances of 'str' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\1775863479.py\u001b[0m:23: TypeError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_fb04578909834ed4801b6f712d55e9b0.py::\u001b[1mtest_non_numeric_input_string\u001b[0m - TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\u001b[31mFAILED\u001b[0m t_fb04578909834ed4801b6f712d55e9b0.py::\u001b[1mtest_non_numeric_input_none\u001b[0m - TypeError: '<' not supported between instances of 'NoneType' and 'int'\n",
      "\u001b[31mFAILED\u001b[0m t_fb04578909834ed4801b6f712d55e9b0.py::\u001b[1mtest_non_numeric_input_mixed\u001b[0m - TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\u001b[31m================================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m11 passed\u001b[0m\u001b[31m in 0.77s\u001b[0m\u001b[31m ===================================\u001b[0m\n",
      "\n",
      "\n",
      "============================================ FAILURES =============================================\n",
      "\u001b[31m\u001b[1m__________________________________ test_non_numeric_input_string __________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_non_numeric_input_string\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grades = [\u001b[33m\"\u001b[39;49;00m\u001b[33ma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m25\u001b[39;49;00m, \u001b[94m30\u001b[39;49;00m, \u001b[94m33\u001b[39;49;00m, \u001b[94m22\u001b[39;49;00m, \u001b[94m28\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           compute_average(grades)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\2611686661.py\u001b[0m:62: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "grades = ['a', 25, 30, 33, 22, 28]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Computes the average of grades, excluding the best and worst grades.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        grades (list): A list of six grades, where each grade is between 18 and 33.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        float: The computed average.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(grades) != \u001b[94m6\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe list must contain exactly six grades.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m grade \u001b[95min\u001b[39;49;00m grades:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mif\u001b[39;49;00m grade < \u001b[94m18\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m grade > \u001b[94m33\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: '<' not supported between instances of 'str' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\1775863479.py\u001b[0m:23: TypeError\n",
      "\u001b[31m\u001b[1m___________________________________ test_non_numeric_input_none ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_non_numeric_input_none\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grades = [\u001b[94mNone\u001b[39;49;00m, \u001b[94m25\u001b[39;49;00m, \u001b[94m30\u001b[39;49;00m, \u001b[94m33\u001b[39;49;00m, \u001b[94m22\u001b[39;49;00m, \u001b[94m28\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           compute_average(grades)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\2611686661.py\u001b[0m:67: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "grades = [None, 25, 30, 33, 22, 28]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Computes the average of grades, excluding the best and worst grades.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        grades (list): A list of six grades, where each grade is between 18 and 33.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        float: The computed average.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(grades) != \u001b[94m6\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe list must contain exactly six grades.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m grade \u001b[95min\u001b[39;49;00m grades:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mif\u001b[39;49;00m grade < \u001b[94m18\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m grade > \u001b[94m33\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: '<' not supported between instances of 'NoneType' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\1775863479.py\u001b[0m:23: TypeError\n",
      "\u001b[31m\u001b[1m__________________________________ test_non_numeric_input_mixed ___________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_non_numeric_input_mixed\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        grades = [\u001b[94m18\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m25\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m30\u001b[39;49;00m, \u001b[94m33\u001b[39;49;00m, \u001b[94m22\u001b[39;49;00m, \u001b[94m28\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mwith\u001b[39;49;00m pytest.raises(\u001b[96mValueError\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">           compute_average(grades)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\2611686661.py\u001b[0m:77: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n",
      "\n",
      "grades = [18, '25', 30, 33, 22, 28]\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute_average\u001b[39;49;00m(grades):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Computes the average of grades, excluding the best and worst grades.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Args:\u001b[39;49;00m\n",
      "    \u001b[33m        grades (list): A list of six grades, where each grade is between 18 and 33.\u001b[39;49;00m\n",
      "    \u001b[33m\u001b[39;49;00m\n",
      "    \u001b[33m    Returns:\u001b[39;49;00m\n",
      "    \u001b[33m        float: The computed average.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(grades) != \u001b[94m6\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mThe list must contain exactly six grades.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m grade \u001b[95min\u001b[39;49;00m grades:\u001b[90m\u001b[39;49;00m\n",
      ">           \u001b[94mif\u001b[39;49;00m grade < \u001b[94m18\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m grade > \u001b[94m33\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "               ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE           TypeError: '<' not supported between instances of 'str' and 'int'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mC:\\Users\\dorot\\AppData\\Local\\Temp\\ipykernel_9108\\1775863479.py\u001b[0m:23: TypeError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info =====================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_fb04578909834ed4801b6f712d55e9b0.py::\u001b[1mtest_non_numeric_input_string\u001b[0m - TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\u001b[31mFAILED\u001b[0m t_fb04578909834ed4801b6f712d55e9b0.py::\u001b[1mtest_non_numeric_input_none\u001b[0m - TypeError: '<' not supported between instances of 'NoneType' and 'int'\n",
      "\u001b[31mFAILED\u001b[0m t_fb04578909834ed4801b6f712d55e9b0.py::\u001b[1mtest_non_numeric_input_mixed\u001b[0m - TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\u001b[31m================================== \u001b[31m\u001b[1m3 failed\u001b[0m, \u001b[32m11 passed\u001b[0m\u001b[31m in 0.77s\u001b[0m\u001b[31m ===================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the test suite\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "def run_tests():\n",
    "    ipytest.run('-vv')  \n",
    "\n",
    "# Running the tests\n",
    "run_tests()\n",
    "\n",
    "#TODO\n",
    "#what is the results of the tests?7\n",
    "#Some tests passed, some failed. The reference solution passed all tests, while the GPT-generated solution failed a few due to incorrect handling of certain edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Generating code with CodeLLAMA\n",
    "\n",
    "In this step, we leverage CodeLLAMA to automatically generate Python code based on specified requirements.\n",
    "\n",
    "In this step, we define a prompt for coding by providing the requirements, the arguments, and the expected returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad17b16bfd544f485db74c1a3b3ea41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load the model and tokenizer from Hugging Face\u001b[39;00m\n\u001b[32m      8\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mcodellama/CodeLlama-13b-Instruct-hf\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Specify the model name\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move model to GPU if available\u001b[39;00m\n\u001b[32m     10\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#TODO\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Define the prompt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:750\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    748\u001b[39m param = param[...]\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    752\u001b[39m     param = param.contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model_name = \"codellama/CodeLlama-13b-Instruct-hf\"  # Specify the model name\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)  # Move model to GPU if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "You are a powerful AI assistant capable of generating code based on specific requirements. Please write Python code based on the following requirements:\n",
    "\n",
    "### Requirements:\n",
    "Write a Python function, named compute_average to compute the average of grades. Six grades are passed as parameters. The average must be performed on four grades, excluding the highest and lowest.\n",
    "    \n",
    "Args:\n",
    "    grades (list): A list of six grades, where each grade is between 18 and 33.\n",
    "    \n",
    "Returns:\n",
    "    float: The computed average.\n",
    "\n",
    "Examples:\n",
    "    compute_average([18, 19, 20, 30, 33, 25]) -> 23.5\n",
    "\n",
    "\n",
    "Function prototype:\n",
    "    def compute_average(grades):\n",
    "    \n",
    "### Code:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate code using the model\n",
    "outputs = model.generate(inputs.input_ids, max_length=300, max_new_tokens=200, temperature = 0.8, num_return_sequences=1)\n",
    "\n",
    "# Decode the output and print the generated code\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Parsing CodeLLAMA results and creating functions\n",
    "\n",
    "In this step, the focus is on processing the output generated by CodeLLAMA to extract meaningful Python functions that meet the given requirements. The raw output from CodeLLAMA often includes additional context or irrelevant information, so it needs to be parsed and cleaned to isolate the functional code. \n",
    "\n",
    "Once the code is extracted, it can be added in our dictionary of source code results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_code = generated_code.split(\"### Code:\")[1].split(\"### Test Case\")[0].strip()\n",
    "\n",
    "print(formatted_code)\n",
    "\n",
    "my_codes[\"CODELLAMA\"] = formatted_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: organizing and analyzing test case results with pytest\n",
    "\n",
    "The code automates the process of running test cases in a Python file using pytest and parsing the results. It defines a function, parse_test_results, that extracts the counts of errors, failures, and passes from a summary line of the pytest output using regular expressions. The script runs the specified test file (test_cases_01.py) via a subprocess call to pytest, capturing its output in plain text. From the output, it locates the summary line containing test results and uses the parse_test_results function to extract key metrics. It calculates the functional correctness ratio as the fraction of passed tests to the total number of tests executed and prints the counts of passed, failed, and errored tests, along with the computed functional correctness ratio. This process provides a clear and automated way to assess the reliability of the tested code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "with open(\"function_01.py\", \"w\") as f:\n",
    "    f.write(my_codes[\"GPT\"])\n",
    "\n",
    "def parse_test_results(result_string):\n",
    "    \"\"\"\n",
    "    Parses the test result string to extract numbers of errors, failures, and passes.\n",
    "\n",
    "    Args:\n",
    "        result_string (str): A string containing test results (e.g., \"3 failed, 11 passed in 0.04s\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with keys 'errors', 'failures', and 'passed' and their respective counts.\n",
    "    \"\"\"\n",
    "    # Regular expressions to match the counts for errors, failures, and passes\n",
    "    errors = re.search(r\"(\\d+)\\s+errors?\", result_string)\n",
    "    failures = re.search(r\"(\\d+)\\s+failed\", result_string)\n",
    "    passes = re.search(r\"(\\d+)\\s+passed\", result_string)\n",
    "\n",
    "    # Extract numbers or default to 0 if not found\n",
    "    return int(errors.group(1)) if errors else 0, int(failures.group(1)) if failures else 0, int(passes.group(1)) if passes else 0\n",
    "\n",
    "# Define the path to your test file\n",
    "test_file = \"test_cases_01.py\"  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO\n",
    "#define code here to write your function generated with GPT in a file \"function_01.py\". This generated function will be called by the test cases in the file test_cases_01.py\n",
    "\n",
    "#with ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the pytest command and capture the output\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", test_file, \"--disable-warnings\", \"--tb=short\", \"-q\", \"--color=no\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Extract test results from the pytest output\n",
    "output_lines = result.stdout.split(\"\\n\")\n",
    "summary_line = next((line for line in output_lines if \"passed\" in line or \"failed\" in line or \"error\" in line), None)\n",
    "result_line = output_lines[-2]\n",
    "\n",
    "errors, failures, passes = parse_test_results(result_line)\n",
    "gpt_functional_correctness = passes/(errors+failures+passes)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"GPT\")\n",
    "print(f\"# Passed: {passes}\")\n",
    "print(f\"# Failed: {failures}\")\n",
    "print(f\"# Errors: {errors}\")\n",
    "print(f\"Functional Correctness Ratio: {gpt_functional_correctness:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: comparing results\n",
    "\n",
    "We now extend our analysis to compare the results provided by different agents.\n",
    "\n",
    "For simplicity, we store the function always on the same file (e.g., function_01.py) and we re-execute the same test file (e.g., test_cases_01.py)\n",
    "\n",
    "We evaluate the results based on the functional correctness ratio, and we select the best option that was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO\n",
    "#define code here to write your function generated with LLAMA in the same file \"function_01.py\". This generated function will be called by the test cases in the file test_cases_01.py\n",
    "\n",
    "#with ...\n",
    "\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"pytest\", test_file, \"--disable-warnings\", \"--tb=short\", \"-q\", \"--color=no\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Extract test results from the pytest output\n",
    "output_lines = result.stdout.split(\"\\n\")\n",
    "summary_line = next((line for line in output_lines if \"passed\" in line or \"failed\" in line or \"error\" in line), None)\n",
    "\n",
    "\n",
    "result_line = output_lines[-2]\n",
    "\n",
    "errors, failures, passes = parse_test_results(result_line)\n",
    "llama_functional_correctness = passes/(errors+failures+passes)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(\"CODELLAMA\")\n",
    "print(f\"# Passed: {passes}\")\n",
    "print(f\"# Failed: {failures}\")\n",
    "print(f\"# Errors: {errors}\")\n",
    "print(f\"Functional Correctness Ratio: {llama_functional_correctness:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Analyzing and comparing the quality of the generated code\n",
    "\n",
    "Static code quality metrics are used to evaluate the structure, readability, and maintainability of code without executing it. Below is a Python program that calculates some common static code quality metrics for a given Python file:\n",
    "\n",
    "- Lines of Code (LOC): The total number of lines in the file.\n",
    "- Comment Density: The percentage of lines that are comments.\n",
    "- Cyclomatic Complexity: A measure of the complexity of a program based on the number of linearly independent paths.\n",
    "- Maintainability Index (MI): a software metric used to predict how maintainable the code is. Its often calculated using a combination of Cyclomatic Complexity (CC), Lines of Code (LOC), and Halstead Volume (HV).\n",
    "\n",
    "The **Radon** tool is a Python package that helps analyze various aspects of code quality, such as Cyclomatic Complexity (CC), Maintainability Index (MI), Raw Metrics (LOC), and Halstead metrics. You can use Radon to analyze the static code quality of your Python code by running it from the command line or through Python code.\n",
    "\n",
    "The tool can be installed by the following command: pip install radon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import radon.complexity as radon_complexity\n",
    "import radon.metrics as radon_metrics\n",
    "import radon.raw as radon_raw\n",
    "\n",
    "def analyze_code_with_radon(file_path):\n",
    "    \"\"\"\n",
    "    Analyzes a Python file and calculates code quality metrics:\n",
    "    - Cyclomatic Complexity (CC)\n",
    "    - Maintainability Index (MI)\n",
    "    - Raw Metrics (LOC, number of functions, etc.)\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Python file to analyze.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        code = file.read()\n",
    "\n",
    "    # Cyclomatic Complexity Analysis (CC)\n",
    "    cc_results = radon_complexity.cc_visit(code)\n",
    "    print(\"Cyclomatic Complexity:\")\n",
    "    for result in cc_results:\n",
    "        print(f\"Function: {result.name}, Complexity: {result.complexity}\")\n",
    "\n",
    "    # Maintainability Index (MI)\n",
    "    maintainability_index = radon_metrics.mi_visit(code, multi=False)  # Set multi=False for single file\n",
    "    print(f\"\\nMaintainability Index: {maintainability_index}\")\n",
    "\n",
    "    # Raw Metrics (LOC, number of functions, etc.)\n",
    "    raw_metrics = radon_raw.analyze(code)  # Use analyse method for raw metrics\n",
    "    print(f\"\\nLines of Code (LOC): {raw_metrics.loc}\")\n",
    "    print(f\"Number of Comments: {raw_metrics.comments}\")\n",
    "    #print(f\"Blank Lines: {raw_metrics.blank_lines}\")\n",
    "\n",
    "    result = {}\n",
    "    result[\"CC\"] = cc_results[0].complexity\n",
    "    result[\"MI\"] = maintainability_index\n",
    "    result[\"LOC\"] = raw_metrics.loc\n",
    "    result[\"Comments\"] = raw_metrics.comments\n",
    "    return result\n",
    "\n",
    "\n",
    "print(my_codes)\n",
    "\n",
    "\n",
    "overall_results = {}\n",
    "\n",
    "\n",
    "#TODO: cycle over all the pairs language, code in the \"my_codes\" dictionary. For each code, save the function in the function_01.py file and:\n",
    "# - execute the radon analysis with the function above\n",
    "# - run the python tests by using the code provided in a previous cell\n",
    "\n",
    "#Example of the final structure of the overall_results dictionary\n",
    "#{'GPT': {'CC': 4, 'MI': 93.26472421003476, 'LOC': 24, 'Comments': 3, 'FunctionalCorrectness': 0.80}, 'CODELLAMA': {'CC': 1, 'MI': 77.16231550361674, 'LOC': 5, 'Comments': 0, 'FunctionalCorrectness': 0.43}}\n",
    "    \n",
    "for language, code in my_codes.items():\n",
    "    with open(\"function_01.py\", \"w\") as f:\n",
    "        print(\"Language:\", language)\n",
    "        print(\"Code:\", code)\n",
    "        f.write(code)\n",
    "\n",
    "results = analyze_code_with_radon(\"function_01.py\")\n",
    "overall_results[language] = results\n",
    "print(overall_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Plotting the results\n",
    "\n",
    "generate a set of bar charts to visualize various software metrics for different programming languages, including Cyclomatic Complexity (CC), Maintainability Index (MI), Lines of Code (LOC), and the number of comments. \n",
    "\n",
    "start by extracting the relevant data from the overall_results dictionary, which contains these metrics for each language. Then, using Matplotlib, create a 3x2 grid of subplots, each one dedicated to a different metric. \n",
    "\n",
    "The first four subplots display the values for CC, MI, LOC, and comments, while the last subplot shows the functional correctness values for two models, GPT and LLaMA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "languages = list(overall_results.keys())  \n",
    "\n",
    "\n",
    "\n",
    "cc_values = [overall_results[language]['CC'] for language in languages]\n",
    "mi_values = [overall_results[language]['MI'] for language in languages]\n",
    "loc_values = [overall_results[language]['LOC'] for language in languages]\n",
    "comments_values = [overall_results[language]['Comments'] for language in languages]\n",
    "functional_correctness = [overall_results[language]['FunctionalCorrectness'] for language in languages]\n",
    "\n",
    "# Create a figure with subplots for each metric\n",
    "fig, axs = plt.subplots(3, 2, figsize=(10, 8))\n",
    "\n",
    "# Plotting CC values\n",
    "axs[0, 0].bar(languages, cc_values, color='skyblue')\n",
    "axs[0, 0].set_title('Cyclomatic Complexity (CC)')\n",
    "axs[0, 0].set_ylabel('CC')\n",
    "\n",
    "# Plotting MI values\n",
    "axs[0, 1].bar(languages, mi_values, color='lightgreen')\n",
    "axs[0, 1].set_title('Maintainability Index (MI)')\n",
    "axs[0, 1].set_ylabel('MI')\n",
    "\n",
    "# Plotting LOC values\n",
    "axs[1, 0].bar(languages, loc_values, color='blue')\n",
    "axs[1, 0].set_title('Lines of Code (LOC)')\n",
    "axs[1, 0].set_ylabel('LOC')\n",
    "\n",
    "# Plotting Comments values\n",
    "axs[1, 1].bar(languages, comments_values, color='lightcoral')\n",
    "axs[1, 1].set_title('Number of Comments')\n",
    "axs[1, 1].set_ylabel('Comments')\n",
    "\n",
    "#plotting functional correctness\n",
    "axs[2, 0].bar(languages, functional_correctness, color='green')\n",
    "axs[2, 0].set_title('Functional correctness')\n",
    "axs[2, 0].set_ylabel('Comments')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Comparing more LLM agents\n",
    "\n",
    "After analyzing a chat engine (GPT) and a HuggingFace model (CodeLLAMA) try to generate additional examples of code with other engines.\n",
    "\n",
    "You can use to this purpose other models provided by HuggingFace, and/or other ready-made chat engines (e.g., Kwen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO generate additional code samples with other LLM engines\n",
    "qwen_chat = \"\"\n",
    "\n",
    "#add the results to the dictionary of codes\n",
    "\n",
    "#re-execute the analysis in the previous code boxes by comparing more languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Additional functions\n",
    "\n",
    "To evaluate the generalizability of the LLM code generation, now modify the functions on which you are applying your analysis.\n",
    "\n",
    "Consider the following requirements:\n",
    "\n",
    "### Railway company\n",
    "\n",
    "A railway company offers the possibility to people under 15 to travel free. The offer is dedicated to groups\n",
    "from 2 to 5 people travelling together.\n",
    "For being eligible to the offer, at least a member of the group must be at least 18 years old. If this condition\n",
    "applies, all the under 15 members of the group travel free, and the others pay the Base Price.\n",
    "The function computeFee receives as parameters basePrice (the price of the ticket), n_passengers (the\n",
    "number of passengers of the group), n_over18 (the number of passengers at least 18 old), n_under15 (the\n",
    "number of passengers under 15 years old). It gives as output the amount that the whole group has to spend. It\n",
    "gives an error if groups are composed of more than 5 persons.\n",
    "double computeFee(double basePrice, int n_passengers, int n_over18, int n_under15);\n",
    "\n",
    "- define test cases for this case study in test_cases_02.py\n",
    "- use a file function_02.py to host the generated results for this function\n",
    "\n",
    "### Bike Race\n",
    "\n",
    "In a bike race, the bikers must complete the entire track within a maximum time, otherwise their race is not\n",
    "valid. The maximum time is computed, for each race, based on the winner's time, on the average speed on the\n",
    "track, and on the category of the track.\n",
    "For tracks of category 'A' (easy tracks) the maximum time is computed as the winner's time increased by 5% if\n",
    "the average speed is lower than 30 km/h (30 included), 10% if the average speed is between 30 and 35 km/h (35\n",
    "included), and 15% if the average speed is higher than 35 km/h.\n",
    "For tracks of category 'B' (normal tracks) the maximum time is computed as the winner's time increased by 20%\n",
    "if the average speed is lower than 30 km/h (30 included), 25% if the average speed is between 30 and 35 km/h\n",
    "(35 included), and 30% if the average speed is higher than 35 km/h.\n",
    "For tracks of category 'C' (hard tracks) the maximum time does not depend on average speed, and is always\n",
    "computed as the winner's time increased by 50%.\n",
    "The function computeMaxTime receives as parameters winner_time (the time of the winner, in minutes),\n",
    "avg_speed (the average speed of the track, in km/h) and track_type (a char, whose valid values are 'A', 'B', or\n",
    "'C'). It gives as output the maximum time, 0 if there are errors in the input.\n",
    "double computeMaxTime(double winner_time, double avg_speed, char track_type)\n",
    "\n",
    "- define test cases for this case study in test_cases_03.py\n",
    "- use a file function_03.py to host the generated results for this function\n",
    "\n",
    "Re-execute the analysis once you have your test cases and adapted prompts. What changes baed on the complexity of the requirements to implement?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
