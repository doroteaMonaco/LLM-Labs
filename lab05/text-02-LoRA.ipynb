{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA: Low-Rank Adaptation of Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to LoRA**\n",
    "\n",
    "LoRA aims to adapt pre-trained language models by adding low-rank matrices to certain weight matrices, reducing the number of parameters that need to be updated. This saves memory and computation, making it ideal for large models. In LoRA, we introduce low-rank matrices to the weights of the model. This allow to train only the low-rank parameters, while the rest of the model remains frozen. In this way, we can adapt the model to a specific task without having to train the entire model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will manually add the low-rank (trainable) matrices to an existing model (frozen). We will use BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/huggingface/peft/issues/41#issuecomment-1404611868\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement LoRA from scratch on a BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement Low-Rank Adaptation (LoRA) on a BERT model from scratch to better understand the concept and its benefits. We will use the `transformers` library to load the pre-trained BERT model and then modify its attention layers to include low-rank matrices. \n",
    "\n",
    "We will then train the modified BERT model on a downstream task to observe the efficiency of LoRA compared to standard fine-tuning. We will use the same training pipeline as proposed in `lab03` on bert to finetune it on a sentiment classification task on the IMDB dataset, so that we can easily compare the results obtained with previous ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `LoRA` class, which will inherits from `nn.Module` -- the base class for all neural network modules in PyTorch. The constructor takes an `original_layer` (e.g., a linear layer from BERT) and a `rank` parameter that determines the rank of the low-rank matrices. It initializes two low-rank matrices `A` and `B`, which will be used for the adaptation. The dimensions of these matrices are determined by the input and output features of the original layer.\n",
    "\n",
    "`A` and `B` are initialized according to the original LoRA paper (https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "The `forward` method defines how the input `x` is processed through the LoRA layer. The input is multiplied by the low-rank matrix `A` to create a low-rank representation, which is then multiplied by the low-rank matrix `B` to obtain the adapted output. Finally, the output of the original layer is combined with the LoRA output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, original_layer, rank=8):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.in_features = original_layer.in_features\n",
    "        self.out_features = original_layer.out_features\n",
    "\n",
    "        # Initialize the Low-rank matrices A and B\n",
    "        self.A = nn.Parameter(torch.zeros(self.in_features, rank))\n",
    "        self.B = nn.Parameter(torch.randn(size=(rank, self.out_features)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The output is the original layer output plus the low-rank adaptation\n",
    "\n",
    "        # LoRA output\n",
    "        ...\n",
    "\n",
    "        # layer output, which combines the original output with the LoRA one \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose a list of modules that we want to adapt with LoRA. We will update all linear layers found in the transformer architecture of BERT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in model.bert.encoder.layer:\n",
    "    layer.attention.self.query = LoRA(layer.attention.self.query)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to make sure that the model is frozen. We simply all layers by going through the list of modules found in the model, and setting the `requires_grad` attribute to `False`.\n",
    "\n",
    "Then, only for LoRA modules, we unfreeze A and B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters except the LoRA parameters\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False \n",
    "    \n",
    "# Unfreeze only LoRA parameters\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, LoRA):\n",
    "        layer.A.requires_grad = True\n",
    "        layer.B.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we are introducing low rank versions for:\n",
    "- query, key, value, output: all 4 converted to two matrices with 768 * 8 parameters (=> 768 * 8 * 2 * 4 = 49,152)\n",
    "- the two ffnn matrices, both 768x3072, converted to 768x8 and 3072x8 => 2*(768 * 8 + 3072 * 8) = 61,440\n",
    "\n",
    "Repeating this for 12 layers, should give us 12 * (49152 + 61440) = 1,327,104\n",
    "\n",
    "We can easily verify if that's the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point onwards, the classic training pipeline can be applied to the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sentiment analysis dataset\n",
    "dataset = load_dataset('imdb')\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(2000))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to compute accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(sample):\n",
    "    return tokenizer(sample['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "num_train_epochs = 1\n",
    "\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Accuracy on the validation set: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Accuracy on the validation set: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using LoRA with Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to implementing Low-Rank Adaptation (LoRA) from scratch, HuggingFace provides an automated way of applying LoRA to models through the **PEFT** library and `LoraConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PEFT (Parameter-Efficient Fine-Tuning)** is a framework within Hugging Face's ecosystem designed to enable efficient fine-tuning of large language models. PEFT supports various parameter-efficient techniques, including LoRA, Prefix Tuning, and Adapter Layers, to adapt pre-trained models to specific tasks without requiring extensive training or memory resources.\n",
    "\n",
    "We create a PEFT configuration object (in this case, `LoraConfig` since we want to apply LoRA). We specify some parameters:\n",
    "- `r` (rank) determines the rank of the low-rank matrices.\n",
    "- `lora_alpha` a scaling parameter used in LoRA\n",
    "- `lora_dropout` dropout rate for the dropout layers introduced in LoRA\n",
    "- `target_modules` a list of module names that we want to adapt with LoRA. In this case, we adapt all linear layers found in the transformer architecture of BERT (the names of the modules can be found upon inspecting the model).\n",
    "\n",
    "We instantiate a `PeftModelForSequenceClassification` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModelForSequenceClassification\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[...],\n",
    ")\n",
    "\n",
    "peft_model = PeftModelForSequenceClassification(model, peft_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look into the model, we find that, indeed, some extra layers have been added (e.g., `lora_A`, `lora_B`). Their behavior is the same as the layers we implemented from scratch in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.bert.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the number of trainable parameters is the one we approximately expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like before, we can now run the training! We will reuse some of the objects already created for the previous part (e.g., functions to compute metrics, datasets, training arguments). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Initialize the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args, # we will recycle the same training arguments as before!\n",
    "    train_dataset=train_dataset, # also datasets, \n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics # and compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(f\"Accuracy on the validation set: {results['eval_accuracy']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
