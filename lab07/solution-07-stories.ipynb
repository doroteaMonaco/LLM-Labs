{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtisuG2Q6lhm"
   },
   "source": [
    "# Exercise: User Story generation\n",
    "\n",
    "User stories are brief, simple descriptions of a feature or requirement from the perspective of an end user. They are used in Agile software development to capture the user’s needs and goals in a way that prioritizes functionality and value. A typical user story follows the format:\n",
    "\n",
    "\"As a [type of user], I want [a specific goal] so that [reason or benefit].\"\n",
    "\n",
    "This format ensures the focus stays on delivering solutions that meet the user’s expectations. User stories guide development by defining clear, actionable tasks for teams while remaining flexible and open to adaptation as the project evolves.\n",
    "\n",
    "Our objective is to create an agent architecture like the following one:\n",
    "\n",
    "1. **Problem Specification (Domain Specification):**\n",
    "The process begins with the definition of a problem description, which outlines the domain or context of the system. This serves as the input to the LLM and sets the foundation for generating user stories. Our problem specification is provided as the input to the process.\n",
    "\n",
    "2. **Role(s) Extraction (\"Who\" Aspect):**\n",
    "A prompt is used to extract roles from the problem description.\n",
    "Roles represent the different types of users or stakeholders involved in the system This step answers \"who\" the user stories are about.\n",
    "\n",
    "3. **Function(s) Extraction (\"What\" Aspect):**\n",
    "Another prompt is used to extract functions associated with each role.\n",
    "Functions describe the specific actions or goals the roles aim to perform within the system. This step answers \"what\" the users want to achieve.\n",
    "\n",
    "4. **Purpose(s) Extraction (\"Why\" Aspect):**\n",
    "A subsequent prompt is used to determine the purposes behind the functions identified. Purposes explain the reasons or motivations behind the desired actions of the roles. This step answers \"why\" the users need the functions.\n",
    "\n",
    "5. **User Stories Generation:**\n",
    "Combining the roles (who), functions (what), and purposes (why), a final prompt generates complete user stories. These user stories take the standard Agile format: \"As a \\<who\\>, I want \\<what\\>, so that \\<why\\>.\"\n",
    "\n",
    "**Feedback Loop and Prompt Chain**:\n",
    "The process is iterative, as each step informs the next and ensures that the roles, functions, and purposes are fully aligned. The LLM is guided by a structured \"prompt chain,\" where prompts are carefully designed to extract specific information at each stage.\n",
    "\n",
    "**Output**:\n",
    "The final output consists of user stories that capture the needs and goals of the system's users in a concise, actionable format. These stories guide development teams in implementing the desired features and functionalities.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243,
     "referenced_widgets": [
      "fe25ee512c184077be919dcd38d43296",
      "99beab2c378044f9833caf3d6b6f3d4e",
      "d3c2cd7f501c4c93ab43730f061273b5",
      "f795283dfaad4da2bb0b3fce64f7f8c2",
      "5ffc14edebc2432f9b95e774167ed333",
      "0adcbace0f6946568311b8010d5c7fc3",
      "2b52835f91344aa396de748f5a68e40e",
      "4d7aec60591343bab6a920285a64112f",
      "6422c15135a24be783734f255088421f",
      "2c93510709bd46c889be8a8872af280e",
      "14d5cc69e04e4fe588144913d220e57f"
     ]
    },
    "id": "hL2Z-R9s3uXY",
    "outputId": "b46b1644-8691-4f6e-bd97-76b8864a48bd"
   },
   "outputs": [],
   "source": [
    "\n",
    "#reference requirements_text\n",
    "\n",
    "requirements_text = \"The proposed platform is designed to enhance the hiking experience for various user groups, including visitors, local guides, platform managers, and hut workers. The platform provides a centralized repository of hiking routes, hut information, and parking facilities. It also enables interactive features such as real-time hike tracking, personalized recommendations, and group hike planning. By combining these capabilities, the platform seeks to foster safe, informed, and collaborative hiking experiences.\\\n",
    "The platform will be deployed as a cloud-based web and mobile application accessible to all stakeholders. The distribution strategy includes an app available on major mobile operating systems, such as iOS and Android, alongside a responsive web interface. It will require an internet connection for features like real-time tracking, notifications, and user authentication, though some offline capabilities, such as pre-downloaded hike information, will also be available.\\\n",
    "User authentication will be role-based, ensuring that only authorized users, such as verified hut workers and platform managers, can access sensitive or administrative features.\\\n",
    "Visitors are the primary users of the platform. They can browse a comprehensive list of hiking trails, filter them based on specific criteria such as difficulty, length, or starting point, and view detailed descriptions. To access advanced features like personalized recommendations, visitors can create user accounts by registering on the platform. Registered users can record their fitness parameters, enabling the system to suggest trails tailored to their capabilities.\\\n",
    "During a hike, visitors can record their progress by marking reference points and sharing their live location through a broadcasting URL. They can also initiate group activities by planning hikes, adding group members, and confirming group participation. The platform allows visitors to start, terminate, and track their hikes, with notifications for unfinished hikes or late group members to ensure safety and accountability.\\\n",
    "Local guides enrich the platform by contributing essential information. They can add detailed descriptions of hikes, parking facilities, and huts, ensuring hikers have accurate and comprehensive data. Local guides also link parking lots and huts to specific trails as starting or arrival points, enhancing the planning process.\\\n",
    "To aid in the visual representation and accessibility of information, local guides can upload pictures of huts and connect these locations directly to hikes. This integration simplifies route planning and helps visitors visualize their journey.\\\n",
    "Platform managers oversee the operational integrity and safety of the platform. They verify new hut worker registrations, ensuring that only authorized personnel can update hut-related data. Managers can also broadcast weather alerts for specific areas, notifying all hikers in those regions through push notifications. This ensures that users stay informed about potentially hazardous conditions.\\\n",
    "The platform manager's role includes maintaining an organized and secure user system while facilitating collaboration between local guides, hut workers, and visitors.\\\n",
    "Hut workers are critical to the maintenance of up-to-date trail and accommodation information. After registering and being verified, hut workers can log into the platform to add or update information about their assigned huts, including uploading pictures and describing the facilities available. They can also monitor and report on the condition of nearby trails, ensuring hikers receive current information.\\\n",
    "Hut workers play a vital role in providing situational updates for hikers. For instance, if a nearby trail is impacted by severe weather or physical obstructions, they can communicate these conditions through the platform. This enhances the safety and preparedness of all hikers relying on the platform.\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Load the tokenizer and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "#we define a method to ask any prompt to llama\n",
    "def ask_llama(prompt, maxl=200, temp=0.7):\n",
    "    \"\"\"\n",
    "    Send a prompt to the Llama model and get a response.\n",
    "\n",
    "    Args:\n",
    "    - prompt (str): The input question or statement to the model.\n",
    "    - max_length (int): The maximum length of the response.\n",
    "    - temperature (float): Controls randomness in the model's output.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's generated response.\n",
    "    \"\"\"\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    inputs.to(device)\n",
    "\n",
    "    # Generate the output\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],  # Tokenized input\n",
    "        max_length=maxl,         # Limit response length to avoid extra text\n",
    "        temperature=temp,        # Lower temperature to reduce randomness\n",
    "        do_sample=True,        # Disable sampling for deterministic output\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
    "    )\n",
    "\n",
    "    # Decode and return the response\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"\"\"\n",
    "System: You are an expert on world capitals.\n",
    "Respond with only the capital city of the given country. Do not repeat the question.\n",
    "\n",
    "Query: What is the capital of France?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = ask_llama(prompt)\n",
    "\n",
    "print(f\"Prompt: {prompt}\\nResponse: {response}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arsD7VxrQ1Hw"
   },
   "source": [
    "## Extracting the Actors\n",
    "\n",
    "To automate the extraction of roles (\"Who\" aspect) from a problem description, we utilize a language model (LLM) to identify key actors or stakeholders involved in the system. Roles, such as \"visitors,\" \"local guides,\" \"platform managers,\" and \"hut workers,\" represent the primary users or contributors to the system and are essential for generating accurate and actionable user stories. This step focuses on using the LLM to extract these roles and then validating the quality of the extraction by comparing the results against a reference dataset.\n",
    "\n",
    "The LLM is provided with a detailed problem description and prompted to extract the roles (actors) associated with the system. The prompt is carefully designed to ensure the LLM captures all relevant **roles** based on the context of the problem. The output is a list of roles that the LLM determines as key stakeholders in the system.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "YOx9Jv_eSzZP",
    "outputId": "786650f3-393e-470f-9758-abd4e0132461"
   },
   "outputs": [],
   "source": [
    "#define three different prompts for extracting users, using the zero-shot, one-shot and few-shot approach. then evaluate the difference in terms of capability to format the output and actual results\n",
    "\n",
    "context_zero_shot = \"\"\"\n",
    "You are an expert requirements analyst specialized in extracting roles for user stories from system requirements. \\\n",
    "Your task is to identify all distinct user roles mentioned in the given requirements text. Roles refer to the specific types of users or stakeholders who interact with the system. \\\n",
    "These roles should align with the personas or actors described in the requirements.\n",
    "\n",
    "Output the list of roles in a clear and concise manner. Do not include duplicates or irrelevant terms.\n",
    "\n",
    "Now, extract the roles from the following requirements text:\n",
    "\"\"\"\n",
    "\n",
    "context_one_shot = \"\"\"\n",
    "You are an expert requirements analyst specialized in extracting roles for user stories from system requirements. \\\n",
    "Your task is to identify all distinct user roles mentioned in the given requirements text. Roles refer to the specific types of users or stakeholders who interact with the system. \\\n",
    "These roles should align with the personas or actors described in the requirements.\n",
    "\n",
    "Output the list of roles in a clear and concise manner. Do not include duplicates or irrelevant terms.\n",
    "\n",
    "**Example:**\n",
    "Requirements Text:\n",
    "\"The application is designed to streamline food delivery operations. Customers can browse menus and place orders. Delivery personnel use the app to view delivery assignments and update order statuses. Restaurant managers can manage their menus and track incoming orders.\"\n",
    "\n",
    "Extracted Roles:\n",
    "1. Customers\n",
    "2. Delivery Personnel\n",
    "3. Restaurant Managers\n",
    "\n",
    "Now, extract the roles from the following requirements text:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "context_few_shots = \"\"\"\n",
    "You are an expert requirements analyst specialized in extracting roles for user stories from system requirements. \\\n",
    "Your task is to identify all distinct user roles mentioned in the given requirements text. Roles refer to the specific types of users or stakeholders who interact with the system. \\\n",
    "These roles should align with the personas or actors described in the requirements.\n",
    "\n",
    "Output the list of roles in a clear and concise manner. Do not include duplicates or irrelevant terms.\n",
    "\n",
    "**Example 1:**\n",
    "Requirements Text:\n",
    "\"The application is designed to streamline food delivery operations. Customers can browse menus and place orders. Delivery personnel use the app to view delivery assignments and update order statuses. Restaurant managers can manage their menus and track incoming orders.\"\n",
    "\n",
    "Extracted Roles:\n",
    "1. Customers\n",
    "2. Delivery Personnel\n",
    "3. Restaurant Managers\n",
    "\n",
    "---\n",
    "\n",
    "**Example 2:**\n",
    "Requirements Text:\n",
    "\"The platform supports the education sector by providing tools for students, teachers, and administrators. Students can access course materials, submit assignments, and track their progress. Teachers can create content, grade submissions, and communicate with students. Administrators oversee user registrations and system performance.\"\n",
    "\n",
    "Extracted Roles:\n",
    "1. Students\n",
    "2. Teachers\n",
    "3. Administrators\n",
    "\n",
    "---\n",
    "\n",
    "**Example 3:**\n",
    "Requirements Text:\n",
    "\"The system provides real-time updates for commuters, transit operators, and city planners. Commuters can check schedules and delays. Transit operators monitor vehicle statuses and manage timetables. City planners analyze system data to improve routes and infrastructure.\"\n",
    "\n",
    "Extracted Roles:\n",
    "1. Commuters\n",
    "2. Transit Operators\n",
    "3. City Planners\n",
    "\n",
    "---\n",
    "\n",
    "Now, extract the roles from the following requirements text:\n",
    "\"\"\"\n",
    "\n",
    "# Concatenate the prompt with the requirements text\n",
    "prompt_zero_shot = context_zero_shot + requirements_text + '\\nAnswer: '\n",
    "prompt_one_shot = context_one_shot + requirements_text + '\\nAnswer: '\n",
    "prompt_few_shots = context_few_shots + requirements_text + '\\nAnswer: '\n",
    "\n",
    "# Call the LLM function for the three prompts and print the results\n",
    "roles_extracted_zero_shot = ask_llama(prompt_zero_shot, maxl=2000)\n",
    "\n",
    "if \"Answer: \" in roles_extracted_zero_shot:\n",
    "    roles_extracted_zero_shot = roles_extracted_zero_shot.split(\"Answer: \")[-1].strip()\n",
    "else:\n",
    "    roles_extracted_zero_shot = \"No roles found.\"\n",
    "\n",
    "print(\"Extracted Roles - zero shot: \\n\", roles_extracted_zero_shot)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "roles_extracted_one_shot = ask_llama(prompt_one_shot, maxl=2000)\n",
    "\n",
    "if \"Answer: \" in roles_extracted_one_shot:\n",
    "    roles_extracted_one_shot = roles_extracted_one_shot.split(\"Answer: \")[-1].strip()\n",
    "else:\n",
    "    roles_extracted_one_shot = \"No roles found.\"\n",
    "\n",
    "print(\"Extracted Roles - one shot: \\n\", roles_extracted_one_shot)\n",
    "\n",
    "\n",
    "\n",
    "roles_extracted_few_shots = ask_llama(prompt_few_shots, maxl=2000)\n",
    "\n",
    "if \"Answer: \" in roles_extracted_few_shots:\n",
    "    roles_extracted_few_shots = roles_extracted_few_shots.split(\"Answer: \")[-1].strip()\n",
    "else:\n",
    "    roles_extracted_few_shots = \"No roles found.\"\n",
    "\n",
    "print(\"Extracted Roles - few shots: \\n\", roles_extracted_few_shots)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5q9_P3oYh8B"
   },
   "source": [
    "While no external dataset is used to refine or influence the role extraction process, we validate the extracted roles against a reference dataset contained in a CSV file named stories.csv. This file includes a specific column listing all the predefined roles associated with the system. These roles serve as the ground truth for validation purposes.\n",
    "\n",
    "\n",
    "To assess the accuracy and completeness of the LLM’s role extraction, we compute the precision and recall metrics by comparing the LLM's output to the ground truth roles:\n",
    "\n",
    "- **Precision**: Measures how many of the roles extracted by the LLM are correct (i.e., exist in the reference dataset).\n",
    "\n",
    "  $P = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives}$\n",
    "\n",
    "\n",
    "- **Recall**: Measures how many of the ground truth roles were successfully identified by the LLM.\n",
    "\n",
    "  $R = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives}$\n",
    "\n",
    "\n",
    "After extracting the roles using the LLM, the results are compared to the roles in the reference column from the CSV file. For each role identified by the LLM:\n",
    "\n",
    "- If the role exists in the reference dataset, it is considered a true positive.\n",
    "- If the role does not exist in the reference dataset, it is marked as a false positive.\n",
    "- Any role in the reference dataset that the LLM fails to identify is considered a false negative.\n",
    "\n",
    "The final output of this process includes:\n",
    "- A list of roles extracted by the LLM.\n",
    "- Precision and recall metrics, which provide a quantitative assessment of the LLM’s performance in role extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQ2WQlindL0M",
    "outputId": "f194b69c-8fba-4a4e-c317-de5271bf3593"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "#define a function to extract unique roles from the .csv files with user stories\n",
    "\n",
    "def extract_unique_roles(file_path):\n",
    "    roles = set()  # Use a set to ensure roles are unique\n",
    "    role_pattern = r\"As a(n)? ([^I]+)\"  # Regex to match the role in the user story\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            if len(row) > 4:  # Ensure the row has enough fields\n",
    "                description = row[4]\n",
    "                match = re.search(role_pattern, description)\n",
    "                if match:\n",
    "                    role = match.group(2).strip()\n",
    "                    roles.add(role)\n",
    "\n",
    "    return list(roles)\n",
    "\n",
    "\n",
    "#this function takes a list of roles, makes all the text lower case, removes spaces, and removes the s at the end\n",
    "\n",
    "def normalize_roles(role_list):\n",
    "    return {role.lower().strip().rstrip('s') for role in role_list}\n",
    "\n",
    "\n",
    "# find the unique roles and put them into a list\n",
    "file_path = 'stories.csv'\n",
    "expected_roles = extract_unique_roles(file_path)\n",
    "print(expected_roles)       #EXPECTED RESULT FOR ROLES\n",
    "\n",
    "\n",
    "#now extract unique roles from the result provided by the LLM\n",
    "#for simplicity, we only consider the few shots prompt for this purpose\n",
    "\n",
    "actual_roles = [role.strip() for role in re.split(r'\\d+\\.\\s', roles_extracted_few_shots) if role.strip()]\n",
    "\n",
    "\n",
    "print(actual_roles)     #ACTUAL RESULT FOR ROLES\n",
    "\n",
    "\n",
    "\n",
    "#write your code here to compute precision and recall\n",
    "#use the normalized roles, i.e., don't consider spaces and 's' at the end of the roles\n",
    "\n",
    "normalized_actual = normalize_roles(actual_roles)\n",
    "normalized_expected = normalize_roles(expected_roles)\n",
    "\n",
    "true_positives = normalized_actual & normalized_expected\n",
    "false_positives = normalized_actual - normalized_expected\n",
    "false_negatives = normalized_expected - normalized_actual\n",
    "\n",
    "precision = len(true_positives) / len(normalized_actual) if normalized_actual else 0\n",
    "recall = len(true_positives) / len(normalized_expected) if normalized_expected else 0\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"True Positives:\", true_positives)\n",
    "print(\"False Positives:\", false_positives)\n",
    "print(\"False Negatives:\", false_negatives)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6OC3Xl4g3wx"
   },
   "source": [
    "## Extracting the Functions\n",
    "\n",
    "In this step, the primary goal is to automate the identification of functions required by the roles identified in the previous step (Role Extraction). The function refers to the tasks or actions that the roles need to perform within the system. To achieve this, a language model (LLM) is used to analyze the problem description, and based on the roles identified, it is prompted to extract the functions those roles desire to perform.\n",
    "\n",
    "For example, after identifying the role of \"visitors\" or \"local guides,\" the LLM is prompted to determine the specific actions these roles want to carry out within the system, such as \"view hikes,\" \"add descriptions,\" or \"manage registrations.\" This extraction process is critical for understanding the functionality needed in the system to meet the requirements of its stakeholders.\n",
    "\n",
    "By leveraging the context and the roles passed from the previous step, the LLM extracts a list of desired functions. This list helps define the operational scope of the system and lays the foundation for generating user stories in subsequent steps. The extracted functions are validated to ensure completeness and alignment with the roles' needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nuw4wXk7krC2",
    "outputId": "3d42f766-1083-4585-f1ac-34109451ff3f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_roles = actual_roles\n",
    "MAX_FUNCTIONS = 15\n",
    "\n",
    "\n",
    "def extract_functions(text):\n",
    "    # Define the start marker we're looking for\n",
    "    start_marker = \"**Functions for Visitors**:\"  # Ensure this matches exactly in the text\n",
    "    start_index = text.find(start_marker)\n",
    "\n",
    "    if start_index == -1:\n",
    "        print(\"Start marker not found!\")\n",
    "        return []  # If the start marker is not found, return an empty list\n",
    "\n",
    "    # Extract the portion of the text starting after \"**Functions for Visitors**\"\n",
    "    text_after_functions = text[start_index + len(start_marker):]\n",
    "\n",
    "    # Clean the extracted text by splitting it into lines\n",
    "    function_lines = text_after_functions.strip().split('\\n')\n",
    "\n",
    "    # Initialize an empty list to store the functions\n",
    "    functions = []\n",
    "\n",
    "    # Iterate over the lines and remove any numbers or dots at the beginning\n",
    "    for line in function_lines[:15]:  # Limit to first 15 functions\n",
    "        # Use regex to remove any unwanted numbers, dots, and extra spaces\n",
    "        function = re.sub(r'[^a-zA-Z\\s]', '', line).strip()\n",
    "        if function:  # Only add non-empty lines\n",
    "            functions.append(function)\n",
    "\n",
    "    return functions\n",
    "\n",
    "\n",
    "\n",
    "def create_user_stories(role, functions):\n",
    "    # Generate user stories in the desired format\n",
    "    user_stories = [(role, f\"As a {role}, I want to {function}\") for function in functions]\n",
    "    return user_stories\n",
    "\n",
    "\n",
    "user_stories = list()\n",
    "\n",
    "for role in current_roles:\n",
    "\n",
    "    print(\"extracting stories for role\", role)\n",
    "\n",
    "    n = 0\n",
    "    while True: \n",
    "        #1. Loop over the list of roles and extract the piece of text from requirements related to that role\n",
    "\n",
    "        n = n + 1\n",
    "        print(\"tentative\", n, \"for\", role)\n",
    "\n",
    "        tmp_prompt=f\"\"\"\n",
    "        You are an expert requirements analyst. Your task is to extract only the relevant **functions** for the given role from the full system requirements provided below. Focus only on the sections and tasks that are related to the **{role}** user. Do not include information related to other roles.\n",
    "\n",
    "        ### Example 1:\n",
    "        **Requirements Text**:\n",
    "        \"The application is designed to streamline food delivery operations. Customers can browse menus and place orders. Delivery personnel use the app to view delivery assignments and update order statuses. Restaurant managers can manage their menus and track incoming orders.\"\n",
    "\n",
    "        **Functions for Delivery personnel**:\n",
    "        1. View delivery assignments.\n",
    "        2. Update order status.\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Example 2:\n",
    "        **Requirements Text**:\n",
    "        \"The platform supports the education sector by providing tools for students, teachers, and administrators. Students can access course materials, submit assignments, and track their progress. Teachers can create content, grade submissions, and communicate with students. Administrators oversee user registrations and system performance.\"\n",
    "\n",
    "        **Functions for Teachers**:\n",
    "        1. Create content.\n",
    "        2. Grade submissions.\n",
    "        3. Communicate with students.\n",
    "\n",
    "        ---\n",
    "\n",
    "        ### Example 3:\n",
    "        **Requirements Text**:\n",
    "        \"The system provides real-time updates for commuters, transit operators, and city planners. Commuters can check schedules and delays. Transit operators monitor vehicle statuses and manage timetables. City planners analyze system data to improve routes and infrastructure.\"\n",
    "\n",
    "        **Functions for City Planners**:\n",
    "        1. Analyze system data.\n",
    "\n",
    "        ---\n",
    "\n",
    "        Now, please review the full system requirements text below.\n",
    "\n",
    "        Please extract only the relevant sections and tasks that describe the features, actions, and functionalities specifically for the **{role}** role. The output should be **concise** and focused solely on the **{role}** user. You should list **at most 15 functions**. Do not include duplicates or functions for other roles other than {role}.\n",
    "\n",
    "        **Requirements Text**:\n",
    "        {requirements_text}\n",
    "\n",
    "        **Functions for {role}**:\n",
    "        \"\"\"\n",
    "\n",
    "        response = ask_llama(tmp_prompt, maxl=2500)\n",
    "        response = response.replace(prompt, '').strip()\n",
    "\n",
    "\n",
    "\n",
    "        functions = extract_functions(response)\n",
    "\n",
    "        print(len(functions))\n",
    "\n",
    "        tmp_user_stories = create_user_stories(role, functions)\n",
    "\n",
    "        print(tmp_user_stories)\n",
    "\n",
    "\n",
    "        if len(functions) > 0 or n > 10:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    user_stories = user_stories + tmp_user_stories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #for function in functions:\n",
    "    #    user_story = f\"As a {role} I want to {function}\"\n",
    "    #    user_stories.append(user_story)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(user_stories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxumxD00nwJu"
   },
   "source": [
    "##Extracting the purposes\n",
    "\n",
    "In this step, the focus is on extracting the purposes behind the functions identified in the previous step. The purpose refers to the underlying reasons or motivations that drive a role to perform a particular function within the system. The language model (LLM) is prompted to identify these reasons by analyzing the problem description and the functions extracted in the previous step.\n",
    "\n",
    "After the functions (what the roles want to do) are identified, the LLM is tasked with understanding and extracting the why behind each function. For example, a role like \"hiker\" may want to \"register for a hike\" (function), but the purpose behind this action might be \"so that they can track their progress\" or \"to ensure they can participate in the hike.\" Similarly, a \"local guide\" may want to \"add a hike description,\" and the purpose could be \"to provide helpful information to other users.\"\n",
    "\n",
    "The extracted purposes help provide a deeper understanding of why each function is important to the roles. These purposes are essential for refining the system’s objectives and ensuring that the design aligns with the actual needs and motivations of the stakeholders. The LLM identifies these purposes by analyzing both the context of the roles and the corresponding functions, ensuring that all relevant purposes are captured to provide clarity for the user stories that will follow in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XY6gJPs-nvel"
   },
   "outputs": [],
   "source": [
    "#we take the pairs of role - function obtained in the previous step\n",
    "#for simplicity, we only consider two user stories at this point\n",
    "#to complete the task, we would require to perform an iteration over all the stories defined - this would be very computationally expensive and is out of the scope of the current exercise\n",
    "\n",
    "user_story1 = user_stories[0]\n",
    "user_story2 = user_stories[1]\n",
    "\n",
    "print(user_story1)\n",
    "\n",
    "role=user_story1[0]\n",
    "function=user_story1[1]\n",
    "\n",
    "print(role)\n",
    "\n",
    "tmp_prompt = f\"\"\"\n",
    "You are an expert requirements analyst. Your task is to complete user stories by adding the purpose (the \"why\") behind a given function based on the full system requirements provided below. The user story format should be: \"As a <role>, I want to <function>, so that <purpose>.\"\n",
    "\n",
    "Here are a few examples of how to generate the full user story:\n",
    "\n",
    "### Example 1:\n",
    "**User Story (without purpose)**: \"As a Customer, I want to browse the menu.\"\n",
    "**Requirements Text**: \"The application allows customers to browse menus and place orders from a variety of restaurants.\"\n",
    "**Completed User Story**: \"As a Customer, I want to browse the menu, so that I can explore and select food items to place an order.\"\n",
    "\n",
    "### Example 2:\n",
    "**User Story (without purpose)**: \"As a Teacher, I want to grade submissions.\"\n",
    "**Requirements Text**: \"Teachers can grade student submissions through the platform's grading tools.\"\n",
    "**Completed User Story**: \"As a Teacher, I want to grade submissions, so that I can evaluate student performance and provide feedback.\"\n",
    "\n",
    "### Example 3:\n",
    "**User Story (without purpose)**: \"As a Transit Operator, I want to manage timetables.\"\n",
    "**Requirements Text**: \"Transit operators can view and manage vehicle schedules, ensuring that vehicles adhere to the set timetables.\"\n",
    "**Completed User Story**: \"As a Transit Operator, I want to manage timetables, so that I can ensure the vehicles run on time and provide reliable service.\"\n",
    "\n",
    "---\n",
    "\n",
    "Now, please complete the following user story by adding the purpose (\"so that <purpose>\") based on the provided requirements text:\n",
    "\n",
    "**User Story (without purpose)**: \"As a {role}, I want to {function}.\"\n",
    "**Requirements Text**: {requirements_text}\n",
    "\n",
    "**Completed User Story**:\n",
    "\"\"\"\n",
    "\n",
    "response = ask_llama(tmp_prompt, maxl=2500)\n",
    "response = response.replace(tmp_prompt, '').strip()\n",
    "full_story = response.split(\"**Completed User Story**:\")[-1]\n",
    "print(full_story)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0adcbace0f6946568311b8010d5c7fc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14d5cc69e04e4fe588144913d220e57f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b52835f91344aa396de748f5a68e40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c93510709bd46c889be8a8872af280e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d7aec60591343bab6a920285a64112f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ffc14edebc2432f9b95e774167ed333": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6422c15135a24be783734f255088421f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "99beab2c378044f9833caf3d6b6f3d4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0adcbace0f6946568311b8010d5c7fc3",
      "placeholder": "​",
      "style": "IPY_MODEL_2b52835f91344aa396de748f5a68e40e",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "d3c2cd7f501c4c93ab43730f061273b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d7aec60591343bab6a920285a64112f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6422c15135a24be783734f255088421f",
      "value": 2
     }
    },
    "f795283dfaad4da2bb0b3fce64f7f8c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c93510709bd46c889be8a8872af280e",
      "placeholder": "​",
      "style": "IPY_MODEL_14d5cc69e04e4fe588144913d220e57f",
      "value": " 2/2 [00:02&lt;00:00,  1.38s/it]"
     }
    },
    "fe25ee512c184077be919dcd38d43296": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99beab2c378044f9833caf3d6b6f3d4e",
       "IPY_MODEL_d3c2cd7f501c4c93ab43730f061273b5",
       "IPY_MODEL_f795283dfaad4da2bb0b3fce64f7f8c2"
      ],
      "layout": "IPY_MODEL_5ffc14edebc2432f9b95e774167ed333"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
