{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder-only architecture - GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will focus on **decoder-only models**, particularly **GPT-2** (Generative Pre-trained Transformer 2), a model designed for text generation.\n",
    "\n",
    "#### **Decoder-Only Models**\n",
    "Decoder-only models, like GPT-2, differ from encoder-decoder models in that they generate text in a **unidirectional (left-to-right)** manner. These models do not have an encoder to process the entire input at once. Instead, they use **autoregressive** generation, predicting the next token based on the preceding context.\n",
    "\n",
    "For the part of this lab, we will use `GPT2LMHeadModel`, a GPT-2 model with a language modeling head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "model_name = 'gpt2'  \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Embeddings Layer**\n",
    "The embeddings layer creates initial representations of the input tokens, encoding both the meaning of the words and their positions within the sequence.\n",
    "\n",
    "- **Word Embeddings** (`wte`): Maps each of the 50,257 vocabulary tokens into a 768-dimensional vector space.\n",
    "- **Position Embeddings** (`wpe`): Adds positional information to the tokens using a learned embedding of size 1024 (representing the maximum sequence length) with 768 dimensions. This allows the model to understand the order of the tokens, as transformers do not have an inherent sense of position.\n",
    "\n",
    "### 2. **Transformer Block (Decoder)**\n",
    "The core of GPT-2 is composed of 12 identical **transformer blocks**, each consisting of several sub-components. These blocks are stacked to process the input text in a sequential, autoregressive manner.\n",
    "\n",
    "Each transformer block includes the following:\n",
    "\n",
    "- **Layer Normalization (ln_1, ln_2)**: Normalization is applied before both the attention mechanism and the feed-forward network to stabilize and speed up training. It ensures that the inputs to the layers have the same mean and variance, using learned parameters.\n",
    "\n",
    "- **Self-Attention Mechanism**\n",
    "\n",
    "  - **Self-Attention Mechanism (attn)**: GPT-2 uses masked self-attention to ensure that the model can only attend to tokens that have already been processed, preventing the model from \"seeing\" future tokens. This makes the model autoregressive.\n",
    "    \n",
    "    - **Query, Key, Value Projections**: The attention mechanism computes the **query (Q)**, **key (K)**, and **value (V)** vectors using **Conv1D layers** (`c_attn`). Notice that (1) when the kernel size is 1, the role of Conv1d is just to do a linear projection, and (2) that under the hood, the Conv1d used in HuggingFace is indeed just a linear projection (see code [here](https://github.com/huggingface/transformers/blob/53fad641cfdb5105e2470bcf3ef17ea8e25cc300/src/transformers/pytorch_utils.py#L87)). In other words, `c_attn` performs a linear transformation and covers the role of $W_q$, $W_k$, and $W_v$. All other operations using Conv1d will follow the same principle.\n",
    "\n",
    "    - **Attention Output**: The output of the attention layer is passed through another **Conv1D layer** (`c_proj`) to project it back to the original 768-dimensional space.\n",
    "\n",
    "- **Feed-Forward Neural Network (mlp)**\n",
    "Each transformer block also contains a fully connected feed-forward neural network that processes the output of the attention mechanism. This is done in two stages:\n",
    "\n",
    "  - **First Linear Transformation (`c_fc`)**: Expands the dimensionality from 768 to a larger intermediate size using a **Conv1D layer**.\n",
    "    \n",
    "  - **Activation Function (`act`)**: Applies the **GELU (Gaussian Error Linear Unit)** activation function, which introduces non-linearity and helps the model capture complex patterns in the data.\n",
    "\n",
    "  - **Second Linear Transformation (`c_proj`)**: Projects the output back down from the intermediate size to 768 dimensions using another **Conv1D layer**.\n",
    "\n",
    "- **Residual Connections**\n",
    "  - GPT-2 uses skip connections around both the self-attention and feed-forward layers, where the input to each sub-layer is added to its output.\n",
    "\n",
    "### 3. **Final Layer Normalization and Language Modeling Head**\n",
    "- After passing through all 12 transformer blocks, a final **LayerNorm** (`ln_f`) is applied to normalize the output before it is passed to the language modeling head for token prediction.\n",
    "\n",
    "- The output of the final transformer block is passed through a **linear layer** (`lm_head`) which maps the 768-dimensional hidden states to the vocabulary size (50,257). This step is essential for converting the hidden representations into predictions for the next token in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"I had called upon my friend, Mr. Sherlock Holmes, one day in the autumn of last year and found him in deep conversation with a very stout, florid-faced, elderly gentleman with fiery red hair.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Autoregressively generate tokens \n",
    "generated_ids = model.generate(input_ids.to(device), max_length=200, do_sample=True, temperature=1.1)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decoder (Masked) Self-Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will focus on the **masked self-attention mechanism** used in the decoder blocks of GPT-2. This mechanism allows the model to attend to previous tokens in an autoregressive manner, ensuring that the model generates text sequentially from left to right without looking ahead to future tokens.\n",
    "\n",
    "A plot of the **self-attention matrix** for a single decoder block in GPT-2 is shown below. The matrix represents the attention weights between different tokens in the input sequence. The **x-axis** and **y-axis** correspond to the tokens in the input sequence. Each position on these axes represents a specific token in the input text.\n",
    "  \n",
    "#### **Masked Self-Attention Behavior:**\n",
    "- As expected in a **decoder-only** model, we observe a clear triangular pattern. Tokens only attend to themselves and the tokens that precede them. For example, the first token (`I`) only attends to itself, the second token (`love`) attends to both itself and the first token, and so on.\n",
    "- The upper-right part of the matrix is empty (dark purple), indicating that the model **masks future tokens** to prevent them from being used in generating the current token. This ensures that GPT-2 maintains its **autoregressive property**, where each token is generated based only on past tokens.\n",
    "\n",
    "- In the image, you can see that certain tokens attend more strongly to previous tokens. For example, the token `favorite` attends heavily to earlier tokens like `Italian` and `food`, as indicated by the brighter colors in the heatmap.\n",
    "- The model tends to attend more to the recent past tokens, which is crucial for maintaining context during text generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model_name = 'gpt2'  # Assicurati di avere il nome del modello corretto\n",
    "model = GPT2Model.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"I love Italian food, my favorite dish is\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate the model output and retrieve attention weights\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  # List of attention matrices from all layers\n",
    "\n",
    "# Visualize the attention weights from the last layer\n",
    "attention_matrix = attentions[-1][0][10].cpu().numpy()  # Last layer, first head\n",
    "\n",
    "# Get the tokens for the input prompt\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "# Plot the attention matrix (last layer, first head) with words as labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "plt.title('Self-Attention Matrix (Last Layer, Tenth Head)')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Token')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to what we have seen in the previous exercise (`01-bert`), GPT-2 uses **positional embeddings** to encode the position of tokens in the input sequence. These embeddings are added to the token embeddings to provide the model with information about the order of the tokens. \n",
    "\n",
    "Also in GPT-2, the positional embeddings are learned during training, allowing the model to capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Extract learned positional embeddings from the model\n",
    "positional_embeddings = model.wpe.weight.detach().cpu().numpy()  # Shape: (1024, 768)\n",
    "\n",
    "# Compute cosine similarity between positional embeddings\n",
    "cosine_sim = cosine_similarity(positional_embeddings)\n",
    "\n",
    "fig, ax = plt.subplot_mosaic(\"\"\"\n",
    "AAA\n",
    "AAA\n",
    "AAA\n",
    "BBB\"\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the cosine similarity heatmap\n",
    "ax[\"A\"].set_aspect('equal')\n",
    "cbar = ax[\"A\"].imshow(cosine_sim, cmap='viridis', vmin=-1, vmax=1)\n",
    "ax[\"A\"].set_xlabel('Vector 1')\n",
    "ax[\"A\"].set_ylabel('Vector 2')\n",
    "ax[\"A\"].axhline(100, c='r', lw=2)\n",
    "# add colorbar\n",
    "fig.colorbar(cbar, ax=ax[\"A\"])\n",
    "\n",
    "\n",
    "# Plot the cosine similarity for a specific row\n",
    "\n",
    "ax[\"B\"].plot(cosine_sim[100], c='r', lw=2)\n",
    "ax[\"B\"].set_xlabel('Vector 1')\n",
    "ax[\"B\"].set_ylabel('Cosine Similarity')\n",
    "ax[\"B\"].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing Sampling Methods:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will explore various **sampling methods** used for text generation in GPT-2. Each sampling method determines how the model selects the next token when generating text. \n",
    "\n",
    "GPT-2 produces a probability distribution over possible tokens for each position in the sequence. The different sampling strategies allow us to choose how we sample from this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set the pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"I had called upon my friend, Mr. Sherlock Holmes, one day in the autumn of last year and found him in deep conversation with a very stout, florid-faced, elderly gentleman with fiery red hair.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set the pad token to eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"I had called upon my friend, Mr. Sherlock Holmes, one day in the autumn of last year and found him in deep conversation with a very stout, florid-faced, elderly gentleman with fiery red hair.\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Greedy Sampling**: Always selects the token with the highest probability at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Sampling: Pick the token with the highest probability\n",
    "def greedy_sampling(model, input_ids, max_length):\n",
    "    input_ids = input_ids.to(device)\n",
    "    for _ in range(max_length):\n",
    "        # NOTE: There's no need to pass an attention mask, since we are passing a single\n",
    "        # input sequence and the model will automatically create the mask.\n",
    "        logits = model(input_ids=input_ids).logits[:, -1, :] # Compute the logits for the last token (1st batch)\n",
    "        next_token = torch.argmax(logits, dim=1).unsqueeze(1) # Get the most likely next token\n",
    "        input_ids = torch.hstack([input_ids, next_token]) # Add the token to the output sequence\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "# Generate and return text using each sampling method\n",
    "max_length = 100  # Max tokens to generate\n",
    "\n",
    "# Greedy Sampling\n",
    "input_length = inputs[\"input_ids\"].shape[-1]\n",
    "output_greedy = greedy_sampling(model, input_ids, max_length)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(output_greedy[0, input_length:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Beam Search**: Expands multiple candidate sequences and selects the best based on their cumulative probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beam Search (simple version with fixed beam width)\n",
    "\n",
    "# NOTE: each score is the log-probability of the sequence\n",
    "# (remember, the sum of log-probabilities is the log-probability of the product)\n",
    "# (i.e., of the entire sentence)\n",
    "def beam_search(model, input_ids, max_length, beam_width=3):\n",
    "    sequences = [(input_ids.to(device), 0)]  # (sequence, score)\n",
    "    for _ in range(max_length):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            logits = model(seq).logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            top_k_probs, top_k_tokens = torch.topk(probs, beam_width, dim=-1) # Get top k tokens\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                candidate_seq = torch.cat([seq, top_k_tokens[:, i].unsqueeze(-1)], dim=-1)  # Add token to sequence\n",
    "                candidate = (candidate_seq, score - torch.log(top_k_probs[:, i]).item()) # Update score by adding -log(prob)\n",
    "                all_candidates.append(candidate) # Add new candidate\n",
    "\n",
    "        # Select top `beam_width` sequences\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
    "\n",
    "        if sequences[0][0][0, -1].item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "    return sequences[0][0]\n",
    "\n",
    "# Beam Search\n",
    "output_beam = beam_search(model, input_ids, max_length, beam_width=5)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(output_beam[0, input_length:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Random Sampling**: Samples tokens randomly according to their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling: Randomly sample a token from the distribution\n",
    "def random_sampling(model, input_ids, max_length):\n",
    "    output = input_ids.to(device)\n",
    "    for _ in range(max_length):\n",
    "        logits = model(output).logits[:, -1, :]\n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # Sample a token from the probability distribution produced by the model\n",
    "\n",
    "        output = torch.cat([output, next_token], dim=-1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return output\n",
    "\n",
    "# Random Sampling\n",
    "output_random = random_sampling(model, input_ids, max_length)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(output_random[0, input_length:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Top-k Sampling**: Samples only from the top `k` most probable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k Sampling: Sample from the top k tokens\n",
    "def top_k_sampling(model, input_ids, max_length, k=50):\n",
    "    output = input_ids.to(device)\n",
    "    for _ in range(max_length):\n",
    "        logits = model(output).logits[:, -1, :]\n",
    "\n",
    "        top_k_logits, top_k_tokens = torch.topk(logits, k, dim=-1)\n",
    "        top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "        sampled_index = torch.multinomial(top_k_probs, num_samples=1)\n",
    "        next_token = top_k_tokens[0, sampled_index] # Select the sampled token\n",
    "\n",
    "        output = torch.cat([output, next_token], dim=-1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "    return output\n",
    "\n",
    "# Top-k Sampling\n",
    "output_top_k = top_k_sampling(model, input_ids, max_length, k=50)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(output_top_k[0, input_length:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Top-p (Nucleus) Sampling**: Samples from the smallest set of tokens whose cumulative probability exceeds `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-p (Nucleus) Sampling: Sample from the smallest set of tokens whose cumulative probability exceeds p\n",
    "def top_p_sampling(model, input_ids, max_length, p=0.9):\n",
    "    output = input_ids.to(device)\n",
    "    for _ in range(max_length):\n",
    "        logits = model(output).logits[:, -1, :]\n",
    "        \n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Cumulative Distribution Function (CDF)\n",
    "        top_p_indices = cumulative_probs <= p\n",
    "\n",
    "        if top_p_indices.sum() == 0:  # Fix for when there are no valid top-p indices\n",
    "            top_p_indices[0] = True  # Ensure at least one token is considered\n",
    "\n",
    "        top_p_probs = F.softmax(sorted_logits[top_p_indices], dim=-1) # Recompute the probabilities\n",
    "        sampled_index = torch.multinomial(top_p_probs, num_samples=1) # Sample from the updated distribution\n",
    "\n",
    "        next_token = sorted_indices[top_p_indices][sampled_index]\n",
    "        output = torch.cat([output, next_token.unsqueeze(0)], dim=-1)\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return output\n",
    "\n",
    "# Top-p (Nucleus) Sampling\n",
    "output_top_p = top_p_sampling(model, input_ids, max_length, p=0.9)\n",
    "print(prompt)\n",
    "print(tokenizer.decode(output_top_p[0, input_length:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Temperature Scaling**: Adjusts the randomness of the token selection by scaling the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Scaling: Adjust the randomness of predictions by scaling the logits\n",
    "def temperature_sampling(model, input_ids, max_length, temperature=0.7):\n",
    "    output = input_ids.to(device)\n",
    "    for _ in range(max_length):\n",
    "        logits = model(output).logits[:, -1, :]\n",
    "        \n",
    "        logits = logits / temperature # Scale the logits by the temperature parameter\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        output = torch.cat([output, next_token], dim=-1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return output\n",
    "\n",
    "# Temperature Sampling\n",
    "print(prompt)\n",
    "print()\n",
    "print(\"Temperature 0.25\")\n",
    "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=0.25)\n",
    "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"Temperature 0.5\")\n",
    "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=0.5)\n",
    "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"Temperature 0.75\")\n",
    "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=0.75)\n",
    "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"Temperature 1.1\")\n",
    "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=1.1)\n",
    "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
    "print()\n",
    "print(\"Temperature 1.5\")\n",
    "output_temperature = temperature_sampling(model, input_ids, max_length, temperature=1.5)\n",
    "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Pre-Implemented Sampling Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face `transformers` library offers built-in implementations for a variety of sampling methods used in text generation , such as the one we have just implemented, through the `generate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text using various sampling methods\n",
    "def generate_text(model, input_ids, max_length, method, **kwargs):\n",
    "\n",
    "    methods = {\n",
    "        \"greedy\": {},\n",
    "        \"beam\": {\"num_beams\": kwargs.get('num_beams', 3), \"early_stopping\": True},\n",
    "        \"random\": {\"do_sample\": True},\n",
    "        \"top_k\": {\"do_sample\": True, \"top_k\": kwargs.get('top_k', 50)},\n",
    "        \"top_p\": {\"do_sample\": True, \"top_p\": kwargs.get('top_p', 0.9)},\n",
    "        \"temperature\": {\"do_sample\": True, \"temperature\": kwargs.get('temperature', 0.7)}\n",
    "    }\n",
    "    return model.generate(input_ids.to(device), max_length=max_length, pad_token_id=tokenizer.eos_token_id, **methods[method])\n",
    "\n",
    "# Generate text using each sampling method\n",
    "max_length = 200  # Increased max_length to generate more tokens\n",
    "\n",
    "# Get the length of the input prompt in tokens\n",
    "input_length = len(tokenizer.encode(prompt, add_special_tokens=False))\n",
    "\n",
    "# Greedy Sampling\n",
    "output_greedy = generate_text(model, inputs['input_ids'], max_length, 'greedy')\n",
    "print(\"Greedy Sampling:\")\n",
    "print(tokenizer.decode(output_greedy[0, input_length:], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "# Beam Search\n",
    "output_beam = generate_text(model, inputs['input_ids'], max_length, 'beam', num_beams=5)\n",
    "print(\"\\nBeam Search:\")\n",
    "print(tokenizer.decode(output_beam[0, input_length:], skip_special_tokens=True))\n",
    "\n",
    "# Random Sampling\n",
    "output_random = generate_text(model, inputs['input_ids'], max_length, 'random')\n",
    "print(\"\\nRandom Sampling:\")\n",
    "print(tokenizer.decode(output_random[0, input_length:], skip_special_tokens=True))\n",
    "\n",
    "# Top-k Sampling\n",
    "k = 50\n",
    "output_top_k = generate_text(model, inputs['input_ids'], max_length, 'top_k', top_k=k)\n",
    "print(f\"\\nTop-{k} Sampling:\")\n",
    "print(tokenizer.decode(output_top_k[0, input_length:], skip_special_tokens=True))\n",
    "\n",
    "# Top-p (Nucleus) Sampling\n",
    "p = 0.9\n",
    "output_top_p = generate_text(model, inputs['input_ids'], max_length, 'top_p', top_p=p)\n",
    "print(f\"\\nTop-{p} (Nucleus) Sampling:\")\n",
    "print(tokenizer.decode(output_top_p[0, input_length:], skip_special_tokens=True))\n",
    "\n",
    "# Temperature Scaling\n",
    "temperature = 0.7\n",
    "output_temperature = generate_text(model, inputs['input_ids'], max_length, 'temperature', temperature=temperature)\n",
    "print(f\"\\nTemperature Sampling ({temperature}):\")\n",
    "print(tokenizer.decode(output_temperature[0, input_length:], skip_special_tokens=True))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
