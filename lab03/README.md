## Lab 03

The third lab is divided into two parts:
- BERT: in this part, you will use BERT, an encoder-only model that is often adopted to solve downstream tasks (e.g., sentence classification). You will explore its architecture, observe how the attention mechanism works in encoder-only models, use it on the original tasks (masked LM, Next Sentence Prediction) and, finally, fine-tune it to solve a new task.
- GPT-2: in this part, you will use GPT-2, a decoder-only model used for next token prediction. You will explore its architecture and masked self-attention (typical of decoders, to preserve causality). You will then use GPT-2 to generate new sequences of tokens, with various sampling policies. 

The following are the files you will use for this lab:
- BERT exercise ([text](./text-01-bert.ipynb)) ([solution](./solution-01-bert.ipynb))
- GPT-2 exercise ([text](./text-02-gpt2.ipynb)) ([solution](./solution-02-gpt2.ipynb))
