{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-only architecture - BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will walk through the basics of the BERT architecture, how its tokenizer works, and how positional encoding is applied. \n",
    "\n",
    "BERT ([Bidirectional Encoder Representations from Transformers](https://arxiv.org/pdf/1810.04805)) is a transformer-based model designed for NLP tasks. It has multiple layers of encoders, each containing:\n",
    "- Self-attention mechanism\n",
    "- Feed-forward neural networks\n",
    "- Layer normalization and residual connections\n",
    "\n",
    "There are different sizes of BERT available:\n",
    "- BERT base: 12 layers (transformer blocks), 768 hidden units, 12 self-attention heads\n",
    "- BERT large: 24 layers, 1024 hidden units, 16 self-attention heads\n",
    "\n",
    "In addition, a cased and an uncased version of BERT are available. The cased version retains the original case of the input text, while the uncased version converts all characters to lowercase.\n",
    "\n",
    "We will focus on the base version of BERT, in uncased form. In HuggingFace, this model is called `bert-base-uncased`.\n",
    "\n",
    "We'll start by visualizing the structure of BERT's encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# TODO: Load pre-trained BERT model\n",
    "model = ...\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Embeddings Layer**\n",
    "The embeddings layer is responsible for creating representations of the input tokens. It includes:\n",
    "\n",
    "- **Word Embeddings**: Maps each of the 30,522 vocabulary tokens to a 768-dimensional vector.\n",
    "- **Position Embeddings**: Adds positional information to each token using a 512-length sequence of 768-dimensional vectors (in other words, BERT supports at most 512 tokens in the input sequence)\n",
    "- **Token Type Embeddings**: Adds embeddings to distinguish between segments (e.g., Sentence A and Sentence B) with 2 different token types (remember: BERT allows passing a pair of sentences, separated by a `[SEP]` token. This is used, we will see, for the Next Sentence Prediction task).\n",
    "- **Layer Normalization and Dropout**: Normalizes the embeddings and applies dropout to prevent overfitting.\n",
    "\n",
    "### 2. **BERT Encoder**\n",
    "BERT's encoder is composed of 12 layers (for BERT base). The layers are implemented in the **BERTLayer** class. Each BERT layer consists of the following components:\n",
    "\n",
    "- **Self-Attention**:\n",
    "  - The self-attention mechanism is divided into multiple heads, each computing its own attention scores using three linear projections: Query (`Q`), Key (`K`), and Value (`V`). As with T5, the 12 attention heads are contained inside of a single liner layer (separately for Q, K, V) of size $12 \\cdot 64 \\times 768$ ($d_k = d_v = 64$).\n",
    "\n",
    "- **Self-Attention Output**:\n",
    "  - The output of the self-attention mechanism is passed through a dense (linear) layer that projects the output (concatenation of the 12 attention heads outputs) back to the original dimension of 768.\n",
    "\n",
    "- **Feed-Forward Neural Network**:\n",
    "  - Each BERT layer includes a feed-forward network that processes each token independently after the self-attention mechanism (we call this feed-forward network \"position-wise\" because, indeed, it is applied to each token indepedenly of the others).\n",
    "  - This feed-forward network consists of two dense layers:\n",
    "    - The **intermediate layer** expands the dimensionality from 768 to 3072 using a linear transformation and applies a **GELU** (Gaussian Error Linear Unit) activation function for non-linearity.\n",
    "    - The **output layer** projects the 3072-dimensional vector back down to 768 dimensions, matching the input size.\n",
    "\n",
    "- **Residual Connections**:\n",
    "  - BERT uses residual (skip) connections around the self-attention and feed-forward components. The input to each sub-layer is added to its output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: print the first layer of the BERT model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERT Tokenization with WordPiece**\n",
    "\n",
    "BERT uses a **WordPiece tokenizer**, a subword tokenization technique. This tokenizer splits words into smaller subwords or tokens, allowing BERT to represent rare words as combinations of common subwords and prefixes.\n",
    "\n",
    "The WordPiece tokenizer splits words based on their frequencies in the training corpus, much like BPE.\n",
    "\n",
    "- **Common Words**: If a word is frequently used in the training corpus, it will likely be stored as a single token. For example, words like \"hello\" or \"sentence\" are common enough to exist in BERT's vocabulary as single tokens.\n",
    "- **Rare or Unknown Words**: For less common or out-of-vocabulary words, the tokenizer will break them into subword units or prefixes. \n",
    "\n",
    "Subword continuation: The WordPiece tokenizer uses the `##` symbol to indicate that a token is a continuation of a previous token. For example, the word \"happened\" might be tokenized as \"happen\" and \"##ed\".\n",
    "\n",
    "Let’s see how the WordPiece tokenizer processes a sample sentence using BERT’s tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sentence = \"hello, this is a sentence!\"\n",
    "\n",
    "# TODO: Tokenize the sentence \n",
    "tokens = ...\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# TODO: Convert tokens to IDs\n",
    "token_ids = ...\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"it was her responsibility to superintend the entire construction process\"\n",
    "\n",
    "sentence = \"hello, this is a sentence!\"\n",
    "\n",
    "# TODO: Tokenize the sentence\n",
    "tokens = ...\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# TODO: Convert tokens to IDs\n",
    "token_ids = ...\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the word \"superintend\", not found in BERT's vocabulary, is split into three subword tokens: `super`, `##int`, and `##end`. \n",
    "\n",
    "Since `super` is the first token for the word, it is not prepended with `##`, whereas the others are. Note that, for instance, tokens `##end` and `end` (representing the self-contained word \"end\") are encoded as different tokens in BERT:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"##end\", \"end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens in BERT\n",
    "\n",
    "BERT uses several special tokens to structure the input and help the model understand the context and relationships between sequences. These tokens play an important role in various NLP tasks such as sentence classification, next sentence prediction, and more.\n",
    "\n",
    "#### 1. **[CLS] Token** (Classification Token)\n",
    "- The `[CLS]` token is added at the beginning of every input sequence. \n",
    "- BERT uses this token as a representation of the entire sequence. The hidden state of the `[CLS]` token after passing through BERT can be used for classification tasks (we are going to use it later!). \n",
    "- In the training of BERT, the output for the `[CLS]` token is used to build the binary classifier for the Next Sentence Prediction task.\n",
    "\n",
    "#### 2. **[PAD] Token** (Padding Token)\n",
    "- The `[PAD]` token is used to pad sequences so they all have the same length within a batch. The attention mechanism then ignores these `[PAD]` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"This is a short sentence.\"\n",
    "\n",
    "# TODO: Tokenize the sentence with padding=True and max_length=10\n",
    "tokens_single = ...\n",
    "\n",
    "# Display tokenized output and input IDs\n",
    "print(\"Single Sentence Tokenization:\")\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens_single['input_ids'][0]))\n",
    "print(\"Input IDs:\", tokens_single['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use `max_length=10` to specify what the maximum length of sentences should be (in tokens). We additionally include `padding='max_length'`, which means that the tokenizer will add padding tokens (`[PAD]`) to the end of the sentence until it reaches the maximum length.\n",
    "\n",
    "(Note that, if we want to truncate the sentences to `max_length` tokens, we should additionally pass `truncation=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **[SEP] Token** (Separator Token)\n",
    "- The `[SEP]` token is used to separate segments (sentences) in BERT's input.\n",
    "- It is especially important for tasks like Next Sentence Prediction (NSP) where BERT needs to understand the relationship between two sentences.\n",
    "- It is also added at the end of the sequence to indicate its termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"But now it is a little longer.\"\n",
    "\n",
    "# TODO: Tokenize the two sentences with padding and max_length of 20\n",
    "# (You can pass a pair of sentences to the tokenizer as the first and\n",
    "# second argument, they will be treateed as a pair of sentences separated by [SEP])\n",
    "tokens_pair = ...\n",
    "\n",
    "# Display tokenized output and input IDs\n",
    "print(\"\\nTwo Sentences Tokenization:\")\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokens_pair['input_ids'][0]))\n",
    "print(\"Input IDs:\", tokens_pair['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now that we are passing two segments (Sentence A and Sentence B), the tokenizer returns also a `token_type_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pair[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encodes information on which of the two sentences each token belongs to. This `token_type_ids` parameter is passed directly to the model's forward method, and the corresponding segment embeddings are added to the token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **[MASK] Token**\n",
    "- The `[MASK]` token is used during training for the Masked Language Modeling (MLM) task, where a percentage of tokens are masked and the model must predict them based on the context.\n",
    "- This token is rarely used during inference but is crucial in pre-training BERT.\n",
    "- The `[MASK]` token (like other special tokens) can be included directly in the input text -- the tokenizer will assign the corresponding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a mask token \n",
    "mask_token = tokenizer.mask_token \n",
    "mask_token_id = tokenizer.convert_tokens_to_ids(mask_token)\n",
    "\n",
    "# Encode a text\n",
    "text = \"I do not know how the sentence will [MASK].\"\n",
    "\n",
    "# TODO: Tokenize the text\n",
    "tokens = ...\n",
    "\n",
    "# TODO: for each token, print the token and its ID\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encodings in Transformers\n",
    "\n",
    "There are two common approaches for implementing *absolute* positional encodings:\n",
    "\n",
    "#### 1. **Static Sinusoidal Positional Encodings**\n",
    "\n",
    "- A fixed vector is assigned to each absolute position in the sequence. For instance, the sinusoidal vectors used in AIAYN.\n",
    "- **Advantages**:\n",
    "  - This method requires no additional parameters or learning during training.\n",
    "  - The same encoding can be applied across sequences of any length without adjustments.\n",
    "- **Disadvantages**:\n",
    "  - Since these encodings are static, they might not be as flexible or as specific as learned positional embeddings, which can adapt better to the data.\n",
    "\n",
    "#### 2. **Learned Positional Embeddings**\n",
    "\n",
    "- Instead of using a fixed, pre-defined encoding, learned positional embeddings are trained as part of the model, similar to how word embeddings are learned.\n",
    "- The model learns a set of positional vectors, where each vector corresponds to a position in the sequence (one vector for position 0, one for position 1, etc.). \n",
    "- **Advantages**:\n",
    "  - The model can change positional encodings to the data it is trained on, potentially capturing positional relationships more effectively.\n",
    "- **Disadvantages**:\n",
    "  - Learned positional embeddings introduce additional parameters, which increases the model size and training complexity.\n",
    "  - The learned embeddings are limited to a maximum sequence length (e.g., 512 for BERT).\n",
    "\n",
    "BERT uses the second approach: **learned positional embeddings**. During the training process, BERT learns a set of positional vectors corresponding to token positions up to a maximum length (typically 512). These embeddings are added to the token embeddings to provide position information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the matrix of cosine similarities between the positional embeddings to understand how the model \"looks\" at positions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load pre-trained BERT model\n",
    "model_name = 'bert-base-uncased'\n",
    "model = ...\n",
    "\n",
    "# TODO: Position embeddings are stored in the embedding layer as 'position_embeddings.weight'\n",
    "positional_encodings = ...\n",
    "\n",
    "# Compute cosine similarity between positional encodings\n",
    "cosine_sim = cosine_similarity(positional_encodings)\n",
    "\n",
    "fig, ax = plt.subplot_mosaic(\"\"\"\n",
    "AAA\n",
    "AAA\n",
    "AAA\n",
    "BBB\"\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot the cosine similarity heatmap\n",
    "ax[\"A\"].set_aspect('equal')\n",
    "cbar = ax[\"A\"].imshow(cosine_sim, cmap='viridis', vmin=-1, vmax=1)\n",
    "ax[\"A\"].set_xlabel('Vector 1')\n",
    "ax[\"A\"].set_ylabel('Vector 2')\n",
    "ax[\"A\"].axhline(100, c='r', lw=2)\n",
    "# add colorbar\n",
    "fig.colorbar(cbar, ax=ax[\"A\"])\n",
    "\n",
    "\n",
    "# Plot the cosine similarity for a specific row\n",
    "\n",
    "ax[\"B\"].plot(cosine_sim[100], c='r', lw=2)\n",
    "ax[\"B\"].set_xlabel('Vector 1')\n",
    "ax[\"B\"].set_ylabel('Cosine Similarity')\n",
    "ax[\"B\"].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we observe the expected trend on the main diagonal (close vectors have similar distances). However, unlike other situations we discussed in class, the other cosine similarities are a bit noisier. This is, in general, to be expected for learned positional encodings. \n",
    "\n",
    "You can see another interesting aspect in the first 128x128 block, which we can report below for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "cbar = ax.imshow(cosine_sim[:128, :128], cmap='viridis', vmin=-1, vmax=1)\n",
    "ax.set_xlabel('Vector 1')\n",
    "ax.set_ylabel('Vector 2')\n",
    "# add colorbar\n",
    "fig.colorbar(cbar, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, for these 128x128 tokens, the matrix is much more structured (and somewhat resembles the one we already observed for GPT-2). \n",
    "\n",
    "A likely explanation for this behavior can be found in BERT's paper:\n",
    "\n",
    "> To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings\n",
    "\n",
    "This means that the first 128 positional tokens are learned for an extensive period of the training, whereas the remaining tokens are learned only for the ifnal 10% of steps. This is likely to produce better quality positional embeddings for the first 128 tokens, which is reflected in the cosine similarity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that there are 2 segment embeddings as a part of BERT (`model.embeddings.token_type_embeddings`). These embeddings are used to distinguish between different segments in the input (e.g., Sentence A and Sentence B in the Next Sentence Prediction task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.token_type_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "\n",
    "We now know enough about BERT and transformer architectures to check where the 110M parameters of BERT base come from.\n",
    "\n",
    "Let's break down the numbers:\n",
    "\n",
    "- Embedding layers:\n",
    "    - Token embeddings: 30,522 tokens x 768 dimensions = 23,440,896 parameters\n",
    "    - Position embeddings: 512 positions x 768 dimensions = 393,216 parameters (almost negligible)\n",
    "    - Segment embeddings: 2 segments x 768 dimensions = 1,536 parameters (definitely negligible)\n",
    "    - Total: 23,440,896+393,216+1,536 = 23,835,648 parameters\n",
    "\n",
    "- Encoder layers (12x):\n",
    "    - $W_Q$, $W_K$, $W_V$: 768 dimensions x 768 dimensions + 768 biases = 590,592 parameters each (x3 = 1,771,776)\n",
    "    - $W_O$: 768 dimensions x 768 dimensions + 768 biases = 590,592 parameters\n",
    "    - Feed-forward network:\n",
    "        - Intermediate layer: 768 dimensions x 3072 dimensions + 3072 biases = 2,362,368 parameters\n",
    "        - Output layer: 3072 dimensions x 768 dimensions + 768 biases = 2,360,064 parameters\n",
    "    - 2 Layer normalizations: 2 x 768 dimensions x 2 = 3,072 parameters (negligible)\n",
    "    - Approx. total: 1,771,776+590,592 + 2,362,368 + 2,360,064 + 3,072 = 7,087,872 parameters\n",
    "\n",
    "Total: 23,835,648 (embedding layers) + 12 x 7,087,872 (encoder layers) = 108,890,112 parameters\n",
    "\n",
    "Which is close to the expected 110M parameters (especially if we include the various heads for various tasks, and some other parameters which we did not include here).\n",
    "\n",
    "To get the answer directly from the model, we can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([ p.numel() for p in model.parameters () ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Pre-Training Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is pre-trained using two primary tasks: **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**. \n",
    "\n",
    "#### 1. **Masked Language Modeling (MLM)**\n",
    "\n",
    "- In the MLM task, a fraction of tokens (15% in the original BERT work) is hidden by being replaced with the special `[MASK]` token. The model then tries to predict the original token based on the surrounding context. \n",
    "- BERT learns to understand the context and fill in the blanks, which helps it develop contextualized embeddings for each word. \n",
    "\n",
    "We can use `BertForMaskedLM` to load the BERT model with the appropriate head for the MLM task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertForNextSentencePrediction\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# TODO: Load pre-trained BERT model for masked language modeling\n",
    "model_mlm = ...\n",
    "\n",
    "print(model_mlm.cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the head has a final output with 30522 units, which will produce the probability distribution across all tokens. The same head is applied to predict all `[MASK]` tokens in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I want to eat a [MASK].\"\n",
    "\n",
    "# TODO: Tokenize the sentence\n",
    "inputs = ...\n",
    "print(inputs[\"input_ids\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # TODO: Get the model outputs \n",
    "    outputs = ...\n",
    "    predictions = ...\n",
    "\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of BERT for the MLM task is 1x9x30522. 1 is (as always) the batch size (we are working with a single sentence). 9 is the number of tokens in the input sequence (including special tokens). 30522 is the size of the vocabulary.\n",
    "\n",
    "Even though we have one output distribution for each input token, we only care about the probabilities for the `[MASK]` tokens. We can extract, for instance, the top 5 most likely tokens for each `[MASK]` token in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "mask = (inputs['input_ids'] == tokenizer.mask_token_id) \n",
    "top_k_tokens = torch.argsort(predictions[mask], descending=True)[0, :top_k] # 0 => only take first mask\n",
    "\n",
    "for token_id, token, logit in zip(top_k_tokens, tokenizer.convert_ids_to_tokens(top_k_tokens), predictions[mask][0, top_k_tokens]):\n",
    "    print(f\"{token} ({token_id}) - logit: {logit:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Next Sentence Prediction (NSP)**\n",
    "\n",
    "- In addition to MLM, BERT is trained to predict whether one sentence logically follows another. This is called the **Next Sentence Prediction (NSP)** task.\n",
    "- BERT takes two sentences (segments) as input and must determine if the second sentence is a continuation of the first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nsp = BertForNextSentencePrediction.from_pretrained(model_name)\n",
    "\n",
    "sentence_a = \"Arthur Conan Doyle is my favorite author\"\n",
    "sentence_b = \"I read all of his books\"\n",
    "\n",
    "# TODO: Tokenize the two sentences \n",
    "inputs_nsp = ...\n",
    "\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs_nsp['input_ids'][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # TODO: Get the model outputs\n",
    "    outputs_nsp = ...\n",
    "    logits = ...\n",
    "    probs = ...\n",
    "\n",
    "print(f\"True: {probs[0,0].item():.6f}, False: {probs[0,1].item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenize the two sentences \n",
    "sentence_c = \"However, it's raining today\"\n",
    "inputs_nsp_false = ...\n",
    "\n",
    "print(\"Tokens:\", tokenizer.convert_ids_to_tokens(inputs_nsp_false['input_ids'][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # TODO: Get the model outputs\n",
    "    outputs_nsp = ...\n",
    "    logits = ...\n",
    "    probs = ...\n",
    "\n",
    "print(f\"True: {probs[0,0].item():.6f}, False: {probs[0,1].item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Encoder Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism is a crucial component of BERT’s encoder architecture. It allows the model to weigh the importance of each token in a sequence relative to all other tokens from both the left and right sides, making it **bidirectional**.\n",
    "\n",
    "#### 1. **Self-Attention Mechanism**\n",
    "\n",
    "In BERT, each token in a sequence attends to all other tokens, including itself. This is known as **self-attention**. The goal is to understand the relationship and relevance of each token within the entire sequence.\n",
    "\n",
    "\n",
    "The attention mechanism allows BERT to dynamically weigh the relevance of each word in the context of the entire sequence. This helps BERT generate contextualized word representations that take into account the entire sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, each head of each layer has its own attention matrix. We can visualize all attention maps with a 12x12 plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load pre-trained BERT model (with `output_attentions=True`) and tokenizer\n",
    "model = ...\n",
    "tokenizer = ...\n",
    "\n",
    "sentence = \"The dog ate the food because it was hungry\"\n",
    "\n",
    "# TODO: Tokenize the sentence\n",
    "inputs = ...\n",
    "\n",
    "# TODO: Get the model outputs\n",
    "outputs = ...\n",
    "\n",
    "attention = outputs.attentions  # This returns a list of attention matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`attention` contains a t uple with all of the attentions (one for each layer). Each attention is a tensor of shape (batch_size, num_heads, sequence_length, sequence_length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(attention):\", len(attention))\n",
    "print(\"attention shape\", attention[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either visualize one specific layer+head, or we can visualize all possible attentions for all heads/layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(attention[1][0, 1].detach().numpy(), cmap='viridis', aspect='equal')\n",
    "ax.set_xticks(np.arange(len(inputs['input_ids'][0])), tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]), rotation=90)\n",
    "ax.set_yticks(np.arange(len(inputs['input_ids'][0])), tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]));\n",
    "\n",
    "ax.set_xlabel(\"Attended token\")\n",
    "ax.set_ylabel(\"Attending token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(12, 12, figsize=(10, 10))\n",
    "\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        ax[i, j].imshow(attention[i][0, j].detach().numpy(), cmap='viridis', aspect='auto')\n",
    "        ax[i, j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that different attention layers and heads pay attention to different aspects. Can you spot heads where the model pays the \"right\" attention to word \"it\"? To what is the model paying the most attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While BERT is pre-trained on general tasks like Masked Language Modeling (MLM) and Next Sentence Prediction (NSP), we can **fine-tune** it on specific tasks. Fine-tuning involves training the pre-trained BERT model on a smaller dataset for a particular NLP task such as sentiment analysis, text classification, named entity recognition, or question answering.\n",
    "\n",
    "The fine-tuning process typically involves:\n",
    "- Adding a task-specific classification head (generally one or two dense layers + non-linearities suffice) on top of the pre-trained BERT model.\n",
    "- Training the entire model (including both the BERT base and the new classification head) on your labeled dataset.\n",
    "- The fine-tuning process adjusts the weights of the model so that it performs well on the new task, while still retaining the knowledge it gained during pre-training.\n",
    "\n",
    "In this case, we’ll fine-tune BERT for a sentiment classification task. The model will be trained to classify movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BertForSequenceClassification` already introduces a classification head on top of the BERT model. We specify the number of classes (`num_labels`, 2 in this case) so that the library will already create the appropriate head for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the pre-trained BERT model (with `num_labels=2`) and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# TODO: set the device to GPU if available and move the model to the device\n",
    "device = ...\n",
    "\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the HuggingFace transformers library, we can use the `Trainer` API to fine-tune the model. The `Trainer` API simplifies the training process by handling the training loop, logging, and evaluation.\n",
    "\n",
    "We can define the training arguments, including the number of epochs, the learning rate, and the batch size. We also specify the evaluation strategy, which will run the evaluation on the validation set at the end of each epoch.\n",
    "\n",
    "We need to define three things before we continue:\n",
    "- an evaluation function that gives a metric of interest (for instance, accuracy)\n",
    "- the training set, which will be used to fine-tune the model\n",
    "- the validation set, which will be used to evaluate the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation function, we implement a `compute_metrics` function that takes as input an object that contains the model's predictions (logits) and the ground truths. From this, we will return a dictionary of metric name, metric value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to compute accuracy\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sentiment classification task, we will fine-tune a pre-trained BERT model on the IMDB dataset. The model will be trained to predict whether a movie review is positive or negative based on the text content.\n",
    "\n",
    "We will use the IMDB dataset: a dataset of movie reviews, with a positive/negative label. The dataset is already available in the HuggingFace datasets library, and is already split into a training and a test set. To keep the training time short, we will only use a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sentiment analysis dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(2000))\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample of our datasets is a dictionary with a `text` field (the review) and a `label` field (0 for negative, 1 for positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # using json to get a nicer indentation\n",
    "\n",
    "print(json.dumps(train_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feeding into a pre-trained BERT model, the text data needs to be tokenized and converted into input features. Let's define a `tokenize_function` that will handle this preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenize the dataset\n",
    "def tokenize_function(sample):\n",
    "    tokenized_input = tokenizer(sample['text'], padding=\"max_length\", truncation=True)\n",
    "    return tokenized_input\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Trainer** is paired with **TrainingArguments**, which define the parameters for training, evaluation, logging, and saving the model’s progress. These components together allow users to easily fine-tune models for downstream tasks with minimal configuration.\n",
    "\n",
    "- **Trainer**: The `Trainer` class provides a high-level interface to train, evaluate, and make predictions using a pre-trained model. It handles tasks like optimization, gradient computation, and model checkpointing.\n",
    "\n",
    "- **TrainingArguments**: This class is used to configure the fine-tuning process. It includes the essential hyperparameters like learning rate, batch size, number of epochs, logging settings and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 32\n",
    "num_train_epochs = 2\n",
    "\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    logging_dir='./logs',  \n",
    "    logging_steps=10,  # \n",
    ")\n",
    "\n",
    "# Initialize the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hugging Face's `transformers` library, after setting up the **Trainer** and **TrainingArguments**, two key methods are used to manage the model’s training and evaluation:\n",
    "\n",
    "- **`trainer.train()`**: This method initiates the fine-tuning process of the pre-trained model. It handles the forward and backward passes, computes gradients, and updates the model's parameters using the specified optimization algorithm. The method processes the data in batches, performs any logging and checkpointing as specified in the `TrainingArguments`, and loops over the dataset for the number of epochs defined. In essence, `train()` automates the entire training cycle.\n",
    "\n",
    "- **`trainer.evaluate()`**: After training, this method is used to assess the performance of the model on a validation or test dataset. It runs the model in evaluation mode, meaning no gradients are computed, and it returns metrics such as accuracy, loss, or other custom metrics defined in the `compute_metrics` function. Evaluation is often performed at the end of each epoch during training, but `evaluate()` can also be called manually to test model performance after training.\n",
    "\n",
    "Together, `train()` and `evaluate()` allow for efficient model fine-tuning, performance tracking, and validation, making it easier to adapt pre-trained models to specific tasks with minimal coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fine-tune the model\n",
    "results = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model\n",
    "results = ...\n",
    "print(f\"Accuracy on the validation set: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Extra stuff!</span>\n",
    "Sometimes (especially when datasets are smaller, or with low hardware resources), we may want to freeze some (or all!) of the BERT layers during fine-tuning. This can be done by setting the `requires_grad` attribute of the model's parameters to `False`. In this way, we use BERT as a feature extractor, and only train the classification head.  You can try addressing the same task as before, but freezing the BERT layers.\n",
    "\n",
    "Note that some additional care may be needed in this case. For instance, you may consider:\n",
    "- Using more data\n",
    "- Using a more complex classification head (multiple layers, dropouts, etc.)\n",
    "- Use a different learning rate\n",
    "- Only freeze part of BERT's layers (e.g. only the first 6-8 layers, and allow fine-tuning the rest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
