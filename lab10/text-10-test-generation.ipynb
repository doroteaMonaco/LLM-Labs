{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case generation with Large Language Models\n",
    "\n",
    "In this series of exercises, we will investigate the use of LLM to generate test cases.\n",
    "\n",
    "### Step 1: Our reference code\n",
    "\n",
    "As opposed to the previous experience with code generation - where we had valid test cases - we assume this time that we have valid solutions for given software requirements. Our task now is to generate test cases for valid code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the same code is saved in the python script function_01.py\n",
    "\n",
    "original_function=\"\"\"def racer_disqualified(times, winner_times, n_penalties, penalties):\n",
    "    \\\"\"\"\n",
    "    Determines if a racer is disqualified based on their times, penalties, and winner times.\n",
    "\n",
    "    Parameters:\n",
    "        times (list of int): List of the racer's times for three events.\n",
    "        winner_times (list of int): List of winner times for the same three events.\n",
    "        n_penalties (int): Number of penalties the racer incurred.\n",
    "        penalties (list of int): List of penalty values.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the racer is disqualified, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If inputs do not meet the required types or constraints.\n",
    "    \\\"\"\"\n",
    "    # Input validation\n",
    "    if not (isinstance(times, list) and len(times) == 3 and all(isinstance(t, int) for t in times)):\n",
    "        raise ValueError(\"times must be a list of three integers.\")\n",
    "\n",
    "    if not (isinstance(winner_times, list) and len(winner_times) == 3 and all(isinstance(wt, int) for wt in winner_times)):\n",
    "        raise ValueError(\"winner_times must be a list of three integers.\")\n",
    "\n",
    "    if not isinstance(n_penalties, int):\n",
    "        raise ValueError(\"n_penalties must be an integer.\")\n",
    "\n",
    "    if not (isinstance(penalties, list) and all(isinstance(p, int) for p in penalties)):\n",
    "        raise ValueError(\"penalties must be a list of integers.\")\n",
    "\n",
    "    if n_penalties != len(penalties):\n",
    "        raise ValueError(\"n_penalties must match the length of the penalties list.\")\n",
    "\n",
    "    disqualified = False\n",
    "    tot_penalties = 0\n",
    "\n",
    "    # Calculate total penalties and check for any excessive penalty\n",
    "    for penalty in penalties:\n",
    "        tot_penalties += penalty\n",
    "        if penalty > 100:\n",
    "            disqualified = True\n",
    "\n",
    "    # Check for disqualification based on total penalties or number of penalties\n",
    "    if tot_penalties > 100 or n_penalties > 5:\n",
    "        disqualified = True\n",
    "\n",
    "    # Check if any time exceeds 1.5 times the corresponding winner time\n",
    "    for i in range(3):\n",
    "        max_time = winner_times[i] * 1.5\n",
    "        if times[i] > max_time:\n",
    "            disqualified = True\n",
    "\n",
    "    return disqualified\"\"\"\n",
    "\n",
    "\n",
    "file_path = \"function_01.py\"\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(original_function)\n",
    "\n",
    "\n",
    "\n",
    "def racer_disqualified(times, winner_times, n_penalties, penalties):\n",
    "    \"\"\"\n",
    "    Determines if a racer is disqualified based on their times, penalties, and winner times.\n",
    "\n",
    "    Parameters:\n",
    "        times (list of int): List of the racer's times for three events.\n",
    "        winner_times (list of int): List of winner times for the same three events.\n",
    "        n_penalties (int): Number of penalties the racer incurred.\n",
    "        penalties (list of int): List of penalty values.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the racer is disqualified, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If inputs do not meet the required types or constraints.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if not (isinstance(times, list) and len(times) == 3 and all(isinstance(t, int) for t in times)):\n",
    "        raise ValueError(\"times must be a list of three integers.\")\n",
    "\n",
    "    if not (isinstance(winner_times, list) and len(winner_times) == 3 and all(isinstance(wt, int) for wt in winner_times)):\n",
    "        raise ValueError(\"winner_times must be a list of three integers.\")\n",
    "\n",
    "    if not isinstance(n_penalties, int):\n",
    "        raise ValueError(\"n_penalties must be an integer.\")\n",
    "\n",
    "    if not (isinstance(penalties, list) and all(isinstance(p, int) for p in penalties)):\n",
    "        raise ValueError(\"penalties must be a list of integers.\")\n",
    "\n",
    "    if n_penalties != len(penalties):\n",
    "        raise ValueError(\"n_penalties must match the length of the penalties list.\")\n",
    "\n",
    "    disqualified = False\n",
    "    tot_penalties = 0\n",
    "\n",
    "    # Calculate total penalties and check for any excessive penalty\n",
    "    for penalty in penalties:\n",
    "        tot_penalties += penalty\n",
    "        if penalty > 100:\n",
    "            disqualified = True\n",
    "\n",
    "    # Check for disqualification based on total penalties or number of penalties\n",
    "    if tot_penalties > 100 or n_penalties > 5:\n",
    "        disqualified = True\n",
    "\n",
    "    # Check if any time exceeds 1.5 times the corresponding winner time\n",
    "    for i in range(3):\n",
    "        max_time = winner_times[i] * 1.5\n",
    "        if times[i] > max_time:\n",
    "            disqualified = True\n",
    "\n",
    "    return disqualified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define some pytest test cases\n",
    "\n",
    "We now setup an environment to run test cases and obtain the coverage of test cases. To start, we define a couple of test cases with the PyTest library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "\n",
    "#TODO \n",
    "#define test cases with pytest to run with ipytest below\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    ipytest.run('-vv')  \n",
    "\n",
    "# Running the tests with ipytests\n",
    "run_tests()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Computing the pass rate\n",
    "\n",
    "The first objective of our analysis is computing the pass rate of the test cases.\n",
    "\n",
    "The pass rate for a test suite is defined as the ratio between the passing test cases and all the test cases executed.\n",
    "\n",
    "Notice that this ratio is computed in the same way as the Functional Correctness when you are comparing generated code against an existing test suite, but there is a subtle difference in what we are measuring: \n",
    "- when we compute functional correctness, we have a correct test suite, and we are verifying if the code complies to requirements by executing the test cases.\n",
    "- when we compute the pass rate, we have correct code, and we are verifying if the test cases comply to the requirements by executing them against the code.\n",
    "\n",
    "For now, we are defining the test cases manually: we make sure that the pass rate is 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import io\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "#TODO\n",
    "# Run the pytest command and capture the output\n",
    "result = \"\"\n",
    "\n",
    "#TODO\n",
    "# Extract test results from the pytest output\n",
    "# parse the results to find passed, failed and errors\n",
    "# (hint: you can use the code from previous lab)\n",
    "\n",
    "\n",
    "errors, failures, passes = (0,0,0)\n",
    "\n",
    "\n",
    "print(f\"# Passed: {passes}\")\n",
    "print(f\"# Failed: {failures}\")\n",
    "print(f\"# Errors: {errors}\")\n",
    "\n",
    "#compute the pass rate of the test cases\n",
    "pass_rate = 0\n",
    "print(f\"Pass Rate: {pass_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compute the coverage\n",
    "\n",
    "To compute the coverage of a test suite over a function or a set of functions, we can use the coverage library.\n",
    "\n",
    "pip install pytest-cov\n",
    "\n",
    "Once we have the coverage module installed, it is possible to launch the coverage by launching the following command line instructions:\n",
    "- coverage run -m pytest test_function_name\n",
    "- coverage report -m\n",
    "\n",
    "In this code section, define multiple subprocess runs to obtain the results of the coverage computation inside a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# Run the pytest coverage run command\n",
    "result = \"\"\n",
    "\n",
    "#TODO\n",
    "# Run the pytest coverage report command\n",
    "result2 = \"\"\n",
    "\n",
    "#TODO\n",
    "#define code to extract the coverage from the coverage report\n",
    "#1) find the line where the function is defined (the line will report the name of the file)\n",
    "#2) extract the coverage\n",
    "\n",
    "coverage = 0.0\n",
    "\n",
    "print(f\"Coverage: {coverage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Introducing mutations\n",
    "\n",
    "To try out mutation testing, produce a set of variants of the function by changing operators and values. Save all these variants in a dictionary of mutations by modifying the text of the function like in the example below.\n",
    "\n",
    "Remeber to introduce a single mutant in each mutated version of the function.\n",
    "\n",
    "**Note**: several tools exist to automate mutation. You can refer to the libraries mutatest and mutpy to generate automatic mutations for test cases written with pytest. In this example, we will introduce mutations manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#in this mutant, the check \"if penalty < 100\" is changed to \"if penalty > 100\"\n",
    "\n",
    "mutant1 = \"\"\"def racer_disqualified(times, winner_times, n_penalties, penalties):\n",
    "    \\\"\"\"\n",
    "    Determines if a racer is disqualified based on their times, penalties, and winner times.\n",
    "\n",
    "    Parameters:\n",
    "        times (list of int): List of the racer's times for three events.\n",
    "        winner_times (list of int): List of winner times for the same three events.\n",
    "        n_penalties (int): Number of penalties the racer incurred.\n",
    "        penalties (list of int): List of penalty values.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the racer is disqualified, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If inputs do not meet the required types or constraints.\n",
    "    \\\"\"\"\n",
    "    # Input validation\n",
    "    if not (isinstance(times, list) and len(times) == 3 and all(isinstance(t, int) for t in times)):\n",
    "        raise ValueError(\"times must be a list of three integers.\")\n",
    "\n",
    "    if not (isinstance(winner_times, list) and len(winner_times) == 3 and all(isinstance(wt, int) for wt in winner_times)):\n",
    "        raise ValueError(\"winner_times must be a list of three integers.\")\n",
    "\n",
    "    if not isinstance(n_penalties, int):\n",
    "        raise ValueError(\"n_penalties must be an integer.\")\n",
    "\n",
    "    if not (isinstance(penalties, list) and all(isinstance(p, int) for p in penalties)):\n",
    "        raise ValueError(\"penalties must be a list of integers.\")\n",
    "\n",
    "    if n_penalties != len(penalties):\n",
    "        raise ValueError(\"n_penalties must match the length of the penalties list.\")\n",
    "\n",
    "    disqualified = False\n",
    "    tot_penalties = 0\n",
    "\n",
    "    # Calculate total penalties and check for any excessive penalty\n",
    "    for penalty in penalties:\n",
    "        tot_penalties += penalty\n",
    "        if penalty < 100:\n",
    "            disqualified = True\n",
    "\n",
    "    # Check for disqualification based on total penalties or number of penalties\n",
    "    if tot_penalties > 100 or n_penalties > 5:\n",
    "        disqualified = True\n",
    "\n",
    "    # Check if any time exceeds 1.5 times the corresponding winner time\n",
    "    for i in range(3):\n",
    "        max_time = winner_times[i] * 1.5\n",
    "        if times[i] > max_time:\n",
    "            disqualified = True\n",
    "\n",
    "    return disqualified\"\"\"\n",
    "\n",
    "\n",
    "#TODO\n",
    "#define additional mutations and combine them in a mutant list\n",
    "mutant2 = \"\"\n",
    "mutant3 = \"\"\n",
    "mutant4 = \"\"\n",
    "mutant5 = \"\" \n",
    "mutants = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calculating Mutation Score\n",
    "\n",
    "Now cycle over the list of mutants. For every mutant, overwrite the function function_01.py and re-execute the test cases. For each mutant you can compute the following outcome:\n",
    "- Mutant killed: one or more test cases failed\n",
    "- Mutant survived: all test cases passed\n",
    "\n",
    "At the end of the iteration over mutants, compute the mutation score:\n",
    "- Mutation score = survived mutants / total number of mutants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the path where to save the mutants\n",
    "\n",
    "file_path = \"function_01.py\"\n",
    "\n",
    "\n",
    "#initialize killed mutants and survived mutants\n",
    "killed_mutants = 0\n",
    "survived_mutants = 0\n",
    "\n",
    "# Iterate over the list of mutants \n",
    "\n",
    "for mutant in mutants:\n",
    "    \n",
    "    #TODO\n",
    "    #overwrite the file with the function with each mutant\n",
    "\n",
    "    \n",
    "    #TODO\n",
    "    #run the test cases and collect the number of passed tests\n",
    "\n",
    "\n",
    "    #TODO\n",
    "    # Extract test results from the pytest output\n",
    "    # parse the results to find passed, failed and errors\n",
    "    # (hint: you can use the code from previous lab)\n",
    "\n",
    "\n",
    "    #TODO\n",
    "    #update the number of survived or killed mutants\n",
    "\n",
    "    pass\n",
    "\n",
    "#TODO\n",
    "#compute the mutation score\n",
    "mutation_score = 0.0\n",
    "\n",
    "print(f\"Mutation score: {round(mutation_score*100, 2)}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Generating tests with LLMs\n",
    "\n",
    "This time, we will consider again at least two alternatives for test case generation:\n",
    "- a model from HuggingFace, e.g., CodeLLAMA\n",
    "- a chat engine, e.g., ChatGPT or Qwen2.5\n",
    "\n",
    "With each engine, we will generate a new test file (e.g., test_function_01_gpt.py, and test_function_01_llama.py), and replicate the pass rate, coverage and mutation analysis performed before with pre-defined test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TODO \n",
    "#obtain test code by using LLAMA or other chat engines, save the results in different files\n",
    "\n",
    "\n",
    "#TODO\n",
    "#append the results on different test files\n",
    "test_files = []\n",
    "\n",
    "\n",
    "#TODO\n",
    "#traverse all the test files saved in the list\n",
    "for test_file in test_files :\n",
    "\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Doing:\", test_file)\n",
    "\n",
    "    #TODO\n",
    "    #restore the original function in function_01.py after mutation analysis is performed\n",
    "\n",
    "\n",
    "    #TODO\n",
    "    #compute pass rate for the given test file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    number_of_tests = 0\n",
    "    pass_rate = 0.0\n",
    "    print(f\"Number of tests: {number_of_tests}\")\n",
    "    print(f\"Pass Rate: {round(pass_rate*100, 2)}\")\n",
    "\n",
    "    #TODO\n",
    "    #compute coverage for the given test file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Coverage: {coverage}%\")\n",
    "    \n",
    "\n",
    "    #re-execute the mutation analysis\n",
    "\n",
    "    survived_mutants = 0\n",
    "    killed_mutants = 0\n",
    "\n",
    "    for mutant in mutants:\n",
    "    \n",
    "        #TODO\n",
    "        #overwrite the file with the function with each mutant\n",
    "        pass\n",
    "\n",
    "    #TODO\n",
    "    #compute the mutation score\n",
    "    mutation_score = 0.0\n",
    "\n",
    "    print(f\"Mutation score: {round(mutation_score*100, 2)}%\")\n",
    "\n",
    "\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
