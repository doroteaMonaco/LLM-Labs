{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Prompt Engineering\n",
    "\n",
    "Let's consider LLAMA as our starting point. In the following, we see a typical prompt feeding and text generation with LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: System: You are an expert on world capitals.\n",
      "Respond with only the capital city of the given country. Do not repeat the question.\n",
      "\n",
      "Query: What is the capital of France?\n",
      "Answer:\n",
      "Paris\n",
      "\n",
      "Query: What is the capital of China?\n",
      "Answer:\n",
      "Beijing\n",
      "\n",
      "Query: What is the capital of Russia?\n",
      "Answer:\n",
      "Moscow\n",
      "\n",
      "Query: What is the capital of Germany?\n",
      "Answer:\n",
      "Berlin\n",
      "\n",
      "Query: What is the capital of Italy?\n",
      "Answer:\n",
      "Rome\n",
      "\n",
      "Query: What\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Assuming model and tokenizer are already loaded\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU if available)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Input prompt - Make it clear that you want only the direct answer without any explanations or options\n",
    "prompt = \"\"\"\n",
    "System: You are an expert on world capitals.\n",
    "Respond with only the capital city of the given country. Do not repeat the question.\n",
    "\n",
    "Query: What is the capital of France?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],  # Tokenized input\n",
    "    max_length=100,         # Limit response length to avoid extra text\n",
    "    temperature=0.7,        # Lower temperature to reduce randomness\n",
    "    do_sample=True,        # Disable sampling for deterministic output\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure the model doesn't go beyond the end token\n",
    "\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "answer = response.split(\"query:\")[-1].strip()\n",
    "print(\"Response:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitz\n",
    "\n",
    "Reference libraries to install: pip install openai pymupdf faiss-cpu scikit-learn\n",
    "\n",
    "PyMuPDF is a Python library that provides tools for working with PDF files (as well as other document formats like XPS, OpenXPS, CBZ, EPUB, and FB2). It's built on the MuPDF library, a lightweight, high-performance PDF and XPS rendering engine. With PyMuPDF, you can perform various tasks like reading, creating, editing, and extracting content from PDFs, images, and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import fitz\n",
    "\n",
    "#open an example pdf\n",
    "doc = fitz.open(\"example2.pdf\")\n",
    "\n",
    "# Extract text from the first page\n",
    "page = doc.load_page(0)\n",
    "text = page.get_text(\"text\")  # Use 'text' mode to get raw text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Text Summarization\n",
    "\n",
    "Let's ask LLAMA to perform a summarization of the example PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        full_text += page.get_text(\"text\") + \"\\n\"\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association.\n"
     ]
    }
   ],
   "source": [
    "#define the prompt to ask for text summarization. \n",
    "text_summarization_prompt = \"Write a briefly summary of the main concepts of the article\"      #define your prompt here\n",
    "text = extract_text_from_pdf(\"example2.pdf\")                          #load here the FULL text of the article\n",
    "p1 =  \"\"\"Question: {PROMPT}. article: {BODY}\\nAnswer: \"\"\".format(PROMPT=text_summarization_prompt, BODY=text)\n",
    "\n",
    "#feed the prompt to llama\n",
    "#print the result of text summarization into bullets\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "inputs = tokenizer(p1, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "r1 = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "answer = r1.split(\"Answer:\")[-1].strip()\n",
    "print(\"Summary:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a System Prompt\n",
    "\n",
    "Llama was trained with a system message that set the context and persona to assume when solving a task. One of the unsung advantages of open-access models is that you have full control over the system prompt in chat applications. This is essential to specify the behavior of your chat assistant –and even imbue it with some personality–, but it's unreachable in models served behind APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information. <</SYS>>\n",
      "Question: Write a briefly summary of the main concepts of the article. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " \n",
      "Answer: \n",
      "The main concepts of the article are: \n",
      "- Milan is an Italian professional football club founded in 1899. \n",
      "- The club competes in the Serie A, the top tier of Italian football. \n",
      "- The stadium, which was built by Milan's second chairman, Piero Pirelli and has been shared with Inter \n",
      "since 1947, is the largest in Italian football, with a total capacity of 75,817. \n",
      "- The club has a long-standing rivalry with Inter, with whom they contest the Derby della Madonnina. \n",
      "- Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "- Silvio Berlusconi's 31-year tenure as Milan president was a standout period in the club's history, as they \n",
      "established themselves as one of Europe's most dominant and successful clubs. \n",
      "- The club won 29 trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. \n",
      "- Milan is home to multiple Ballon d'Or winners, and three of the club's players, Marco van Basten, Ruud \n",
      "Gullit, and Frank Rijkaard, were ranked in the top three on the podium for the 1988 Ballon d'Or. \n",
      "- Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In\n"
     ]
    }
   ],
   "source": [
    "#default standard system message from the Hugging Face blog to the prompt from above\n",
    "system_prompt = \"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information. <</SYS>>\"\n",
    "\n",
    "#concatenate the system prompt with your pront and get the response\n",
    "p2 =  \"\"\"{SYS}\\nQuestion: {PROMPT}. article: {BODY} \\nAnswer:\"\"\".format(SYS=system_prompt, PROMPT=text_summarization_prompt, BODY=text)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(p2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "r2 = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the System prompt\n",
    "\n",
    "With Llama we have full control over the system prompt. The following experiment will instruct Llama to assume the persona of a researcher tasked with writing a concise brief.\n",
    "\n",
    "Apply the following changes the original system prompt:\n",
    "- Use the researcher persona and specify the tasks to summarize articles.\n",
    "- Remove safety instructions; they are unnecessary since we ask Llama to be truthful to the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful assistant. \n",
      "    When summarizing an article or providing key points, you must:\n",
      "    1. Use **bullet points** to list the main ideas.\n",
      "    2. **Bullet points must be formatted like this:**\n",
      "       - First key point goes here.\n",
      "       - Second key point goes here.\n",
      "       - Third key point goes here.\n",
      "    3. Each bullet point should be one short sentence.\n",
      "    4. Keep bullet points clear, concise, and easy to read.\n",
      "    5. Make sure every distinct idea from the article is listed as a separate bullet point.\n",
      "    6. Avoid paragraphs; only bullet points should be used in the output.\n",
      "<</SYS>>\n",
      "Question: Summarize the article below into bullet points. Format bullet points exactly as described above. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " \n",
      "Answer (use bullet points exactly like this: - point 1): \n",
      "- Point 1: \n",
      "- Point 2: \n",
      "- Point 3: \n",
      "- Point 4: \n",
      "- Point 5: \n",
      "- Point 6: \n",
      "- Point 7: \n",
      "- Point 8: \n",
      "- Point 9: \n",
      "- Point 10: \n",
      "- Point 11: \n",
      "- Point 12: \n",
      "- Point 13: \n",
      "- Point 14: \n",
      "- Point 15: \n",
      "- Point 16: \n",
      "- Point 17: \n",
      "- Point 18: \n",
      "- Point 19: \n",
      "- Point 20: \n",
      "- Point 21: \n",
      "- Point 22: \n",
      "- Point 23: \n",
      "- Point 24: \n",
      "- Point 25: \n",
      "- Point 26: \n",
      "- Point 27: \n",
      "- Point 28: \n",
      "- Point 29: \n",
      "- Point 30: \n",
      "- Point 31: \n",
      "- Point 32: \n",
      "- Point 33: \n",
      "- Point 34: \n",
      "- Point 35: \n",
      "- Point 36: \n",
      "- Point 37: \n",
      "- Point 38: \n",
      "- Point 39: \n",
      "- Point 40: \n",
      "- Point 41: \n",
      "- Point 42: \n",
      "- Point 43: \n",
      "- Point 44: \n",
      "- Point 45: \n",
      "- Point 46: \n",
      "- Point 47: \n",
      "- Point 48: \n",
      "- Point 49: \n",
      "- Point 50:\n"
     ]
    }
   ],
   "source": [
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful assistant. \n",
    "    When summarizing an article or providing key points, you must:\n",
    "    1. Use **bullet points** to list the main ideas.\n",
    "    2. **Bullet points must be formatted like this:**\n",
    "       - First key point goes here.\n",
    "       - Second key point goes here.\n",
    "       - Third key point goes here.\n",
    "    3. Each bullet point should be one short sentence.\n",
    "    4. Keep bullet points clear, concise, and easy to read.\n",
    "    5. Make sure every distinct idea from the article is listed as a separate bullet point.\n",
    "    6. Avoid paragraphs; only bullet points should be used in the output.\n",
    "<</SYS>>\"\"\"\n",
    "\n",
    "text_summarization_prompt2 = \"Summarize the article below into bullet points. Format bullet points exactly as described above\"\n",
    "\n",
    "p3 =  \"\"\"{SYS}\\nQuestion: {PROMPT}. article: {BODY} \\nAnswer (use bullet points exactly like this: - point 1):\"\"\".format(SYS=new_system_prompt, PROMPT=text_summarization_prompt2, BODY=text)\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "inputs = tokenizer(p3, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=500,\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "r3 = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought prompting\n",
    "\n",
    "Chain-of-thought is when a prompt is being constructed using a previous prompt answer. For our use case to extract information from text, we will first ask Llama what the article is about and then use the response to ask a second question: what problem does [what the article is about] solve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      "\n",
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Provide straight to the point answers, without bullet points.\n",
      "<</SYS>>\n",
      "Question: How many titles did AC Milan win?. article: - Point 1: \n",
      "- Point 2: \n",
      "- Point 3: \n",
      "- Point 4: \n",
      "- Point 5: \n",
      "- Point 6: \n",
      "- Point 7: \n",
      "- Point 8: \n",
      "- Point 9: \n",
      "- Point 10: \n",
      "- Point 11: \n",
      "- Point 12: \n",
      "- Point 13: \n",
      "- Point 14: \n",
      "- Point 15: \n",
      "- Point 16: \n",
      "- Point 17: \n",
      "- Point 18: \n",
      "- Point 19: \n",
      "- Point 20: \n",
      "- Point 21: \n",
      "- Point 22: \n",
      "- Point 23: \n",
      "- Point 24: \n",
      "- Point 25: \n",
      "- Point 26: \n",
      "- Point 27: \n",
      "- Point 28: \n",
      "- Point 29: \n",
      "- Point 30: \n",
      "- Point 31: \n",
      "- Point 32: \n",
      "- Point 33: \n",
      "- Point 34: \n",
      "- Point 35: \n",
      "- Point 36: \n",
      "- Point 37: \n",
      "- Point 38: \n",
      "- Point 39: \n",
      "- Point 40: \n",
      "- Point 41: \n",
      "- Point 42: \n",
      "- Point 43: \n",
      "- Point 44: \n",
      "- Point 45: \n",
      "- Point 46: \n",
      "- Point 47: \n",
      "- Point 48: \n",
      "- Point 49: \n",
      "- Point 50: \n",
      "Answer:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Provide straight to the point answers, without bullet points.\n",
      "<</SYS>>\n",
      "Question: How many titles did AC Milan win?. article: - Point 1: \n",
      "- Point 2: \n",
      "- Point 3: \n",
      "- Point 4: \n",
      "- Point 5: \n",
      "- Point 6: \n",
      "- Point 7: \n",
      "- Point 8: \n",
      "- Point 9: \n",
      "- Point 10: \n",
      "- Point 11: \n",
      "- Point 12: \n",
      "- Point 13: \n",
      "- Point 14: \n",
      "- Point 15: \n",
      "- Point 16: \n",
      "- Point 17: \n",
      "- Point 18: \n",
      "- Point 19: \n",
      "- Point 20: \n",
      "- Point 21: \n",
      "- Point 22: \n",
      "- Point 23: \n",
      "- Point 24: \n",
      "- Point 25: \n",
      "- Point 26: \n",
      "- Point 27: \n",
      "- Point 28: \n",
      "- Point 29: \n",
      "- Point 30: \n",
      "- Point 31: \n",
      "- Point 32: \n",
      "- Point 33: \n",
      "- Point 34: \n",
      "- Point 35: \n",
      "- Point 36: \n",
      "- Point 37: \n",
      "- Point 38: \n",
      "- Point 39: \n",
      "- Point 40: \n",
      "- Point 41: \n",
      "- Point 42: \n",
      "- Point 43: \n",
      "- Point 44: \n",
      "- Point 45: \n",
      "- Point 46: \n",
      "- Point 47: \n",
      "- Point 48: \n",
      "- Point 49: \n",
      "- Point 50: \n",
      "Answer: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#define a prompt to ask what the article is about\n",
    "\n",
    "#r3 is already ok to know what the article is about. we just split in two parts since we know the format of the response and we take only the data of interest\n",
    "\n",
    "\n",
    "start_phrase = \"Answer (use bullet points exactly like this: - point 1):\"\n",
    "r4 = r3.split(start_phrase, 1)[1].strip()\n",
    "\n",
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    Provide straight to the point answers, without bullet points.\n",
    "<</SYS>>\"\"\"\n",
    "\n",
    "\n",
    "#now embed the result of the previous prompt in a new prompt to ask some additional information (i.e., the year of foundation). you have to split the original prompt to create a new one\n",
    "\n",
    "additional_info_prompt = \"\"\"How many titles did AC Milan win?\"\"\"\n",
    "\n",
    "\n",
    "p5 =  \"\"\"{SYS}\\nQuestion: {PROMPT}. article: {BODY} \\nAnswer:\"\"\".format(SYS=new_system_prompt, PROMPT=additional_info_prompt, BODY=r4)\n",
    "\n",
    "\n",
    "print(\"PROMPT:\\n\")\n",
    "print(p5)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "inputs = tokenizer(p5, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=500,\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "# Decode the response into human-readable text\n",
    "r5 = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "print(r5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating JSONs with Llama\n",
    "\n",
    "Llama needs precise instructions when asking it to generate JSON. In essence, here is what works for me to get valid JSON consistently:\n",
    "\n",
    "- Explicitly state — “ All output must be in valid JSON. Don’t add explanation beyond the JSON” in the system prompt.\n",
    "- Add an “explanation” variable to the JSON example. Llama enjoys explaining its answers. Give it an outlet.\n",
    "- Use the JSON as part of the instruction. See the “in_less_than_ten_words” example below.\n",
    "Change “write the answer” to “output the answer.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    When providing the answer, follow the JSON formatting instructions carefully.\n",
      "<</SYS>>\n",
      "Question: Summarize the article below. Output must be in valid JSON like the following example {{\"topic\": topic, \"explanation\": [in_less_than_ten_words]}}. Output must include only JSON.. article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " \n",
      "Answer: {{\"topic\": \"Milan\", \"explanation\": [\"The club was founded in 1899 by a group of students from the Milan Polytechnic. The club was named after the Italian city of Milan, and the team's colours were red and black. The team's first game was a friendly against the Italian national team in 1900. The team's first league game was against Inter Milan in 1901, and they played their first official match against the Italian national team in 1903. The team's first trophy was the 1903–04 Torneo di Milano, which they won. The team's first international game was against the Italian national team in 1903. The team's first European game was against the Scottish national team in 1904. The team's first league title was the 1905–06 season, and the team's first European title was the 1908–09 European Cup Winners' Cup.\"\"]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#example addition to a prompt to deal with jsons\n",
    "json_prompt_addition = \"Output must be in valid JSON like the following example {{\\\"topic\\\": topic, \\\"explanation\\\": [in_less_than_ten_words]}}. Output must include only JSON.\"\n",
    "\n",
    "\n",
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    When providing the answer, follow the JSON formatting instructions carefully.\n",
    "<</SYS>>\"\"\"\n",
    "\n",
    "text_summarization_prompt3 = \"Summarize the article below. \" + json_prompt_addition\n",
    "#now generate a prompt by correctly concatenating the system prompt, the json prompt instruction, and an article\n",
    "p6 = \"\"\"{SYS}\\nQuestion: {PROMPT}. article: {BODY} \\nAnswer:\"\"\".format(SYS=new_system_prompt, PROMPT=text_summarization_prompt3, BODY=text)\n",
    "\n",
    "inputs = tokenizer(p6, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=500,\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response\n",
    ")\n",
    "\n",
    "\n",
    "r6 = tokenizer.decode(output[0], skip_special_tokens=True).strip()  \n",
    "print(r6)\n",
    "\n",
    "#compare the difference between the prompt with the formatting instruction and a regular prompt without formatting instructions. is there any difference?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-Many Shot Learning Prompting\n",
    "\n",
    "One-to-Many Shot Learning is a term that refers to a type of machine learning problem where the goal is to learn to recognize many different classes of objects from only one or a few examples of each class. For example, if you have only one image of a cat and one image of a dog, can you train a model to distinguish between cats and dogs in new images? This is a challenging problem because the model has to generalize well from minimal data (source)\n",
    "\n",
    "Important points about the prompts:\n",
    "\n",
    "- The system prompt includes the instructions to output the answer in JSON.\n",
    "- The prompt consists of an one-to-many shot learning section that starts after ```<</SYS>>``` and ends with ```</s>```.  See the prompt template below will make it easier to understand.\n",
    "- The examples are given in JSON because the answers need to be JSON.\n",
    "- The JSON allows defining the response with name, type, and explanation.\n",
    "- The prompt question start with the second ```<s>[INST]``` and end with the last ```[/INST]```\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "SYSTEM MESSAGE\n",
    "<</SYS>>\n",
    "EXAMPLE QUESTION [/INST]\n",
    "EXAMPLE ANSWER(S)\n",
    "</s>\n",
    "<s>[INST]  \n",
    "QUESTION\n",
    "[/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Provide straight to the point answers, without bullet points.\n",
      "    Output must be in the following format, by describing names and descriptions extracted by the article = [    {{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},    {{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},    {{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},    {{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},    {{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},    ]. Output must include only this.\n",
      "<</SYS>>\n",
      "Question: Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan.. \n",
      "article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " \n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=300) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>> You are a helpful, respectful and honest assistant.     Always answer as helpfully as possible, while being safe. Your answers should not include any harmful,     unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses     are socially unbiased and positive in nature. If a question does not make any sense, or is not factually     coherent, explain why instead of answering something not correct. If you don't know the answer to a question,     please don't share false information.\n",
      "    Provide straight to the point answers, without bullet points.\n",
      "    Output must be in the following format, by describing names and descriptions extracted by the article = [    {{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},    {{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},    {{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},    {{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},    {{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},    ]. Output must include only this.\n",
      "<</SYS>>\n",
      "Question: Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan.. \n",
      "article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA Champions League titles. During the \n",
      "1991–92 season, the club notably achieved the feat of being the first team to win the Serie A title \n",
      "without losing a single game. Milan is home to multiple Ballon d'Or winners, and three of the club's \n",
      "players, Marco van Basten, Ruud Gullit, and Frank Rijkaard, were ranked in the top three on the \n",
      "podium for the 1988 Ballon d'Or, an unprecedented achievement in the history of the prize. \n",
      " \n",
      "Domestically, Milan has won 19 league titles, 5 Coppa Italia titles and 7 Supercoppa Italiana titles. In \n",
      "international competitions, Milan is Italy's most successful club. The club has won seven European \n",
      "Cup/Champions League titles, making them the competition's second-most successful team behind \n",
      "Real Madrid, and further honours include five UEFA Super Cups, two UEFA Cup Winners' Cups, a joint \n",
      "record two Latin Cups, a joint record three Intercontinental Cups and one FIFA Club World Cup. \n",
      " \n",
      "Milan is one of the wealthiest clubs in Italian and world football.[20] It was a founding member of the \n",
      "now-defunct G-14 group of Europe's leading football clubs as well as its replacement, the European \n",
      "Club Association. \n",
      " \n",
      "\n",
      " \n",
      "Answer: \n",
      "<</SYS>> \n",
      "Question: Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan.. \n",
      "article: Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947, is the \n",
      "largest in Italian football, with a total capacity of 75,817. The club has a long-standing rivalry with Inter, \n",
      "with whom they contest the Derby della Madonnina, one of the most followed derbies in football. \n",
      " \n",
      "Milan has spent its entire history in Serie A with the exception of the 1980–81 and 1982–83 seasons. \n",
      "Silvio Berlusconi’s 31-year tenure as Milan president was a standout period in the club's history, as \n",
      "they established themselves as one of Europe's most dominant and successful clubs. Milan won 29 \n",
      "trophies during his tenure, securing multiple Serie A and UEFA\n"
     ]
    }
   ],
   "source": [
    "#describe all the main nouns in the example.pdf article\n",
    "new_system_prompt = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    Provide straight to the point answers, without bullet points.\n",
    "    Output must be in the following format, by describing names and descriptions extracted by the article = [\\\n",
    "    {{\"name\": \"semiconductor\", \"type\": \"industry\", \"explanation\": \"Companies engaged in the design and fabrication of semiconductors and semiconductor devices\"}},\\\n",
    "    {{\"name\": \"NBA\", \"type\": \"sport league\", \"explanation\": \"NBA is the national basketball league\"}},\\\n",
    "    {{\"name\": \"Ford F150\", \"type\": \"vehicle\", \"explanation\": \"Article talks about the Ford F150 truck\"}},\\\n",
    "    {{\"name\": \"Ford\", \"type\": \"company\", \"explanation\": \"Ford is a company that built vehicles\"}},\\\n",
    "    {{\"name\": \"John Smith\", \"type\": \"person\", \"explanation\": \"Mentioned in the article\"}},\\\n",
    "    ]. Output must include only this.\n",
    "<</SYS>>\"\"\"\n",
    "\n",
    "text_summarization_prompt3 = \"Summarize the article below. Provide the different main aspects of AC Milan in JSON format in the format of the above example. Provide many different info about Milan.\"\n",
    "\n",
    "#now build the prompt following the template described above\n",
    "p7 =  \"\"\"{SYS}\\nQuestion: {PROMPT}. \\narticle: {BODY} \\nAnswer:\"\"\".format(SYS=new_system_prompt, PROMPT=text_summarization_prompt3, BODY=text)\n",
    "\n",
    "print(p7)\n",
    "\n",
    "inputs = tokenizer(p7, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "# Generate a response\n",
    "output = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=500,\n",
    "    max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "    top_k=10,  # Sampling parameters\n",
    "    num_return_sequences=1,  # Generate a single response\n",
    "    temperature=0.7,  # Adjust randomness\n",
    "    repetition_penalty=1.0,  # Reduce repetition\n",
    "    length_penalty=1.0,  # Penalize longer responses\n",
    "    eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "    early_stopping=True  # Stop once the model generates the response   \n",
    ")\n",
    "r7 = tokenizer.decode(output[0], skip_special_tokens=True).strip()  \n",
    "print(r7)\n",
    "\n",
    "#compare the response of the prompt described above and a zero-shot prompt. Are there any differences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: RAG (Retrieval-Augmented-Generation)\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) is a powerful framework in Natural Language Processing (NLP) that enhances the performance of language models by combining traditional generative models with external knowledge retrieval. This hybrid approach allows models to retrieve relevant information from a large corpus (like a database or document collection) and incorporate this information into the generation process. It is particularly useful when a model needs to answer questions, generate content, or provide explanations based on real-time or domain-specific data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 0 ---\n",
      "\n",
      "Associazione Calcio Milan, commonly referred to as AC Milan or simply Milan, is an Italian \n",
      "professional football club based in Milan, Lombardy. Founded in 1899, the club competes in the Serie \n",
      "A, the top tier of Italian football. In its early history, Milan played its home games in different grounds \n",
      "around the city before moving to its current stadium, the San Siro, in 1926. The stadium, which was \n",
      "built by Milan's second chairman, Piero Pirelli and has been shared with Inter Milan since 1947,\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 1 ---\n",
      "\n",
      "XIWU: A BASIS FLEXIBLE AND LEARNABLE LLM FOR HIGH\n",
      "ENERGY PHYSICS\n",
      "Zhengde Zhang1, Yiyu Zhang1, Haodong Yao1, Jianwen Luo2, Rui Zhao1, Bo Huang1, Jiameng Zhao3,1,\n",
      "Yipu Liao1, Ke Li1, Lina Zhao1, Jun Cao1, Fazhi Qi1,∗, and Changzheng Yuan1,†\n",
      "1Institute of High Energy Physics, Chinese Academy of Sciences, Beijing 100049, China\n",
      "2School of Physical Sciences, University of Chinese Academy of Sciences, Beijing 100049, China\n",
      "3School of Computer and Artificial Intelligence, ZhengZhou University, Henan 450\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 2 ---\n",
      "\n",
      "1\n",
      "Towards Greener LLMs: Bringing Energy-Efficiency\n",
      "to the Forefront of LLM Inference\n",
      "Jovan Stojkovic, Esha Choukse†, Chaojie Zhang†, Inigo Goiri†, Josep Torrellas\n",
      "University of Illinois at Urbana-Champaign\n",
      "†Microsoft Azure Research - Systems\n",
      "Abstract—With the ubiquitous use of modern large language\n",
      "models (LLMs) across industries, the inference serving for these\n",
      "models is ever expanding. Given the high compute and memory\n",
      "requirements of modern LLMs, more and more top-of-the-\n",
      "line GPUs are being \n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 3 ---\n",
      "\n",
      "Offline Energy-Optimal LLM Serving: Workload-Based Energy\n",
      "Models for LLM Inference on Heterogeneous Systems\n",
      "Grant Wilkins\n",
      "gfw27@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Srinivasan Keshav\n",
      "sk818@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Richard Mortier\n",
      "rmm1002@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "ABSTRACT\n",
      "The rapid adoption of large language models (LLMs) has led to\n",
      "significant advances in natural language processing and text gener-\n",
      "ation. However, the energy consumed throu\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 4 ---\n",
      "\n",
      "Posted on 12 Aug 2024 — CC-BY-NC-SA 4 — https://doi.org/10.36227/techrxiv.172348951.12175366/v1 — e-Prints posted on TechRxiv are preliminary reports that are not peer reviewed. They should not b...\n",
      "Optimizing LLM Inference Clusters for Enhanced Performance and\n",
      "Energy Eﬃciency\n",
      "Soka Hisaharo1, Yuki Nishimura1, and Aoi Takahashi1\n",
      "1Aﬃliation not available\n",
      "August 12, 2024\n",
      "Abstract\n",
      "The growing demand for eﬃcient and scalable AI solutions has driven research into optimizing the performance and energy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 5 ---\n",
      "\n",
      "Applied Energy 367 (2024) 123431\n",
      "Available online 16 May 2024\n",
      "0306-2619/© 2024 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\n",
      "EPlus-LLM: A large language model-based computing platform for \n",
      "automated building energy modeling \n",
      "Gang Jiang a, Zhihao Ma a, Liang Zhang b, Jianli Chen a,* \n",
      "a The University of Utah, United States \n",
      "b The University of Arizona, United States   \n",
      "H I G H L I G H T S  \n",
      "• This study represents the pione\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 6 ---\n",
      "\n",
      "DynamoLLM: Designing LLM Inference Clusters\n",
      "for Performance and Energy Efficiency\n",
      "Jovan Stojkovic, Chaojie Zhang†, ´I˜nigo Goiri†, Josep Torrellas, Esha Choukse†\n",
      "University of Illinois at Urbana-Champaign\n",
      "†Microsoft Azure Research - Systems\n",
      "Abstract—The rapid evolution and widespread adoption of\n",
      "generative large language models (LLMs) have made them a\n",
      "pivotal workload in various applications. Today, LLM inference\n",
      "clusters receive a large number of queries with strict Service\n",
      "Level Objectives (SL\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 7 ---\n",
      "\n",
      "Large Language Model (LLM)-enabled In-context\n",
      "Learning for Wireless Network Optimization: A\n",
      "Case Study of Power Control\n",
      "Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu,\n",
      "Xue Liu, Fellow, IEEE, and Charlie Zhang, Fellow, IEEE.\n",
      "Abstract—Large language model (LLM) has recently been\n",
      "considered a promising technique for many fields. This work\n",
      "explores LLM-based wireless network optimization via in-context\n",
      "learning. To showcase the potential of LLM technologies, we\n",
      "consider the base station (BS) power\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 8 ---\n",
      "\n",
      "Hybrid Heterogeneous Clusters Can Lower the Energy\n",
      "Consumption of LLM Inference Workloads\n",
      "Grant Wilkins\n",
      "gfw27@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Srinivasan Keshav\n",
      "sk818@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "Richard Mortier\n",
      "rmm1002@cam.ac.uk\n",
      "University of Cambridge\n",
      "Cambridge, UK\n",
      "ABSTRACT\n",
      "Both the training and use of Large Language Models (LLMs) require\n",
      "large amounts of energy. Their increasing popularity, therefore,\n",
      "raises critical concerns regarding the energy efficiency a\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 9 ---\n",
      "\n",
      "Can Private LLM Agents Synthesize Household Energy\n",
      "Consumption Data?\n",
      "Mahathir Almashor∗\n",
      "mahathir.almashor@csiro.au\n",
      "CSIRO Energy\n",
      "Sydney, Australia\n",
      "Yusuke Miyashita∗\n",
      "yusuke.miyashita@csiro.au\n",
      "CSIRO Energy\n",
      "Melbourne, Australia\n",
      "Sam West∗\n",
      "sam.west@csiro.au\n",
      "CSIRO Energy\n",
      "Newcastle, Australia\n",
      "Thi Van Dai Dong∗\n",
      "thivandai.dong@csiro.au\n",
      "CSIRO Energy\n",
      "Wollonggong, Australia\n",
      "ABSTRACT\n",
      "Reproducible science requires easy access to data, especially with\n",
      "the rise of data-driven and increasingly complex models used\n",
      "\n",
      "\n",
      "\n",
      "--- Text from PDF file 10 ---\n",
      "\n",
      "Democratizing Energy Management with LLM-Assisted Optimization\n",
      "Autoformalism\n",
      "Ming Jin, Bilgehan Sel, Fnu Hardeep, Wotao Yin\n",
      "Abstract— This paper introduces a method for personaliz-\n",
      "ing energy optimization using large language models (LLMs)\n",
      "combined with an optimization solver. This approach, termed\n",
      "human-guided optimization autoformalism, translates natural\n",
      "language speciﬁcations into optimization problems, enabling\n",
      "LLMs to handle various user-speciﬁc energy-related tasks. It\n",
      "allows for nuanced \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "#TODO:  Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    print(\"\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in range(len(doc)):\n",
    "        page_num = doc.load_page(page)\n",
    "        full_text += page_num.get_text()\n",
    "    return full_text\n",
    "\n",
    "# Extract text from all uploaded PDF files\n",
    "pdf_texts = {}\n",
    "# your code here...\n",
    "directory_path = \".\"\n",
    "pdf_files = glob.glob(os.path.join(directory_path, \"*.pdf\"))\n",
    "n = 0\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_texts[n] = extract_text_from_pdf(pdf_file)\n",
    "    n += 1\n",
    "\n",
    "#Display the text from all the PDF files\n",
    "for pdf_file, text in pdf_texts.items(): \n",
    "    print(f\"--- Text from PDF file {pdf_file} ---\\n\")\n",
    "    print(text[:500])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an index of vectors to represent the documents\n",
    "\n",
    "To perform efficient searches, we need to convert our text data into numerical vectors. To do so, we will use the first step of the BERT transformer.\n",
    "\n",
    "Since our full pdf files are very long to be fed as input into BERT, we perform a step in which we create a structure where we associate a document number to its abstract, and in a separate dictionary we associate a document number to its full text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import the Bert pretrained model from the transformers library\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "#initialization of the dictionary of abstracts. Substitute this with the abstracts of the 10 papers considered as sources for RAG\n",
    "#(we could use functions to read the PDFs to \"cut\" the abstracts from the papers. For simplicity reasons, we will copy and paste them)\n",
    "abstracts_dict = {\n",
    "    0: \"\"\"Large Language Models (LLMs) are undergoing a period of rapid updates and changes, with state-\n",
    "of-the-art (SOTA) model frequently being replaced. When applying LLMs to a specific scientific\n",
    "field, it’s challenging to acquire unique domain knowledge while keeping the model itself advanced.\n",
    "To address this challenge, a sophisticated large language model system named as Xiwu has been\n",
    "developed, allowing you switch between the most advanced foundation models and quickly teach the\n",
    "model domain knowledge. In this work, we will report on the best practices for applying LLMs in the\n",
    "field of high-energy physics (HEP), including: a seed fission technology is proposed and some data\n",
    "collection and cleaning tools are developed to quickly obtain domain AI-Ready dataset; a just-in-time\n",
    "learning system is implemented based on the vector store technology; an on-the-fly fine-tuning system\n",
    "has been developed to facilitate rapid training under a specified foundation model.\n",
    "The results show that Xiwu can smoothly switch between foundation models such as LLaMA, Vicuna,\n",
    "ChatGLM and Grok-1. The trained Xiwu model is significantly outperformed the benchmark model\n",
    "on the HEP knowledge Q&A and code generation. This strategy significantly enhances the potential\n",
    "for growth of our model’s performance, with the hope of surpassing GPT-4 as it evolves with the\n",
    "development of open-source models. This work provides a customized LLM for the field of HEP,\n",
    "while also offering references for applying LLM to other fields, the corresponding codes are available\n",
    "on Github https://github.comzhang/zhengde0225/Xiwu.\"\"\",\n",
    "    1: \"\"\"\n",
    "Abstract—With the ubiquitous use of modern large language\n",
    "models (LLMs) across industries, the inference serving for these\n",
    "models is ever expanding. Given the high compute and memory\n",
    "requirements of modern LLMs, more and more top-of-the-\n",
    "line GPUs are being deployed to serve these models. Energy\n",
    "availability has come to the forefront as the biggest challenge for\n",
    "data center expansion to serve these models. In this paper, we\n",
    "present the trade-offs brought up by making energy efficiency\n",
    "the primary goal of LLM serving under performance SLOs.\n",
    "We show that depending on the inputs, the model, and the\n",
    "service-level agreements, there are several knobs available to\n",
    "the LLM inference provider to use for being energy efficient.\n",
    "We characterize the impact of these knobs on the latency,\n",
    "throughput, as well as the energy. By exploring these trade-\n",
    "offs, we offer valuable insights into optimizing energy usage\n",
    "without compromising on performance, thereby paving the way\n",
    "for sustainable and cost-effective LLM deployment in data center\n",
    "environments\n",
    "    \"\"\",\n",
    "    2: \"\"\"\n",
    "The rapid adoption of large language models (LLMs) has led to\n",
    "significant advances in natural language processing and text gener-\n",
    "ation. However, the energy consumed through LLM model infer-\n",
    "ence remains a major challenge for sustainable AI deployment. To\n",
    "address this problem, we model the workload-dependent energy\n",
    "consumption and runtime of LLM inference tasks on heterogeneous\n",
    "GPU-CPU systems. By conducting an extensive characterization\n",
    "study of several state-of-the-art LLMs and analyzing their energy\n",
    "and runtime behavior across different magnitudes of input prompts\n",
    "and output text, we develop accurate (𝑅2 >0.96)energy and run-\n",
    "time models for each LLM. We employ these models to explore\n",
    "an offline, energy-optimal LLM workload scheduling framework.\n",
    "Through a case study, we demonstrate the advantages of energy\n",
    "and accuracy aware scheduling compared to existing best practices\n",
    "    \"\"\",\n",
    "    3: \"\"\"\n",
    "The growing demand for efficient and scalable AI solutions has driven research into optimizing the performance and energy\n",
    "efficiency of computational infrastructures. The novel concept of redesigning inference clusters and modifying the GPT-Neo\n",
    "model offers a significant advancement in addressing the computational and environmental challenges associated with AI\n",
    "deployment. By developing a novel cluster architecture and implementing strategic architectural and algorithmic changes,\n",
    "the research achieved substantial improvements in throughput, latency, and energy consumption. The integration of advanced\n",
    "interconnect technologies, high-bandwidth memory modules, and energy-efficient power management techniques, alongside\n",
    "software optimizations, enabled the redesigned clusters to outperform baseline models significantly. Empirical evaluations\n",
    "demonstrated superior scalability, robustness, and environmental sustainability, emphasizing the potential for more sustainable\n",
    "AI technologies. The findings underscore the importance of balancing performance with energy efficiency and provide a robust\n",
    "framework for future research and development in AI optimization. The research contributes valuable insights into the design\n",
    "and deployment of more efficient and environmentally responsible AI systems.\n",
    "1\n",
    "\"\"\",\n",
    "    4: \"\"\"\n",
    "Establishing building energy models (BEMs) for building design and analysis poses significant challenges due to demanding modeling efforts, expertise to use simulation software, and building science knowledge in practice. These make building modeling labor-intensive, hindering its widespread adoptions in building development. Therefore, to overcome these challenges in building modeling with enhanced automation in modeling practice, this paper proposes Eplus-LLM (EnergyPlus-Large Language Model) as the auto-building modeling platform, building on a fine-tuned large language model (LLM) to directly translate natural language description of buildings to established building models of various geometries, occupancy scenarios, and equipment loads. Through fine-tuning, the LLM (i.e., T5) is customized to digest natural language and simulation demands from users and convert human descriptions into EnergyPlus modeling files. Then, the Eplus-LLM platform realizes the automated building modeling through invoking the API of simulation software (i.e., the EnergyPlus engine) to simulate the auto-generated model files and output simulation results of interest. The validation process, involving four different types of prompts, demonstrates that Eplus-LLM reduces over 95% modeling efforts and achieves 100% accuracy in establishing BEMs while being robust to interference in usage, including but not limited to different tones, misspells, omissions, and redundancies. Overall, this research serves as the pioneering effort to customize LLM for auto-modeling purpose (directly build-up building models from natural language), aiming to provide a user-friendly human-AI interface that significantly reduces building modeling efforts. This work also further facilitates large-scale building model efforts, e.g., urban building energy modeling (UBEM), in modeling practice.\n",
    "\"\"\",\n",
    "5: \"\"\"\n",
    "The rapid evolution and widespread adoption of\n",
    "generative large language models (LLMs) have made them a\n",
    "pivotal workload in various applications. Today, LLM inference\n",
    "clusters receive a large number of queries with strict Service\n",
    "Level Objectives (SLOs). To achieve the desired performance,\n",
    "these models execute on power-hungry GPUs causing the in-\n",
    "ference clusters to consume large amount of energy and, conse-\n",
    "quently, result in excessive carbon emissions. Fortunately, we find\n",
    "that there is a great opportunity to exploit the heterogeneity in\n",
    "inference compute properties and fluctuations in inference work-\n",
    "loads, to significantly improve energy-efficiency. However, such\n",
    "a diverse and dynamic environment creates a large search-space\n",
    "where different system configurations (e.g., number of instances,\n",
    "model parallelism, and GPU frequency) translate into different\n",
    "energy-performance trade-offs. To address these challenges, we\n",
    "propose DynamoLLM, the first energy-management framework\n",
    "for LLM inference environments. DynamoLLM automatically\n",
    "and dynamically reconfigures the inference cluster to optimize for\n",
    "energy and cost of LLM serving under the service’s performance\n",
    "SLOs. We show that at a service-level, DynamoLLM conserves\n",
    "53 percent energy and 38 percent operational carbon emissions, and reduces\n",
    "61 percent cost to the customer, while meeting the latency SLOs\n",
    "\"\"\",\n",
    "6: \"\"\"\n",
    "Large language model (LLM) has recently been\n",
    "considered a promising technique for many fields. This work\n",
    "explores LLM-based wireless network optimization via in-context\n",
    "learning. To showcase the potential of LLM technologies, we\n",
    "consider the base station (BS) power control as a case study,\n",
    "a fundamental but crucial technique that is widely investigated\n",
    "in wireless networks. Different from existing machine learning\n",
    "(ML) methods, our proposed in-context learning algorithm relies\n",
    "on LLM’s inference capabilities. It avoids the complexity of\n",
    "tedious model training and hyper-parameter fine-tuning, which is\n",
    "a well-known bottleneck of many ML algorithms. Specifically, the\n",
    "proposed algorithm first describes the target task via formatted\n",
    "natural language, and then designs the in-context learning\n",
    "framework and demonstration examples. After that, it considers\n",
    "two cases, namely discrete-state and continuous-state problems,\n",
    "and proposes state-based and ranking-based methods to select\n",
    "appropriate examples for these two cases, respectively. Finally, the\n",
    "simulations demonstrate that the proposed algorithm can achieve\n",
    "comparable performance as conventional deep reinforcement\n",
    "learning (DRL) techniques without dedicated model training or\n",
    "fine-tuning. Such an efficient and low-complexity approach has\n",
    "great potential for future wireless network optimization.\n",
    "Index Terms—Large language model, in-context learning, net-\n",
    "work optimization, transmission power contro\n",
    "\"\"\",\n",
    "7: \"\"\"\n",
    "Both the training and use of Large Language Models (LLMs) require\n",
    "large amounts of energy. Their increasing popularity, therefore,\n",
    "raises critical concerns regarding the energy efficiency and sus-\n",
    "tainability of data centers that host them. This paper addresses the\n",
    "challenge of reducing energy consumption in data centers running\n",
    "LLMs. We propose a hybrid data center model that uses a cost-based\n",
    "scheduling framework to dynamically allocate LLM tasks across\n",
    "hardware accelerators that differ in their energy efficiencies and\n",
    "computational capabilities. Specifically, our workload-aware strat-\n",
    "egy determines whether tasks are processed on energy-efficient\n",
    "processors or high-performance GPUs based on the number of in-\n",
    "put and output tokens in a query. Our analysis of a representative\n",
    "LLM dataset, finds that this hybrid strategy can reduce CPU+GPU\n",
    "energy consumption by 7.5 percent compared to a workload-unaware\n",
    "baseline\n",
    "\"\"\",\n",
    "8: \"\"\"\n",
    "Reproducible science requires easy access to data, especially with\n",
    "the rise of data-driven and increasingly complex models used within\n",
    "energy research. Too often however, the data to reconstruct and\n",
    "verify purported solutions in publications is hidden due to some\n",
    "combination of commercial, legal, and sensitivity issues. This early\n",
    "work presents our initial efforts to leverage the recent advance-\n",
    "ments in Large Language Models (LLMs) to create usable and share-\n",
    "able energy datasets. In particular, we’re utilising their mimicry of\n",
    "human behaviors, with the goal of extracting and exploring syn-\n",
    "thetic energy data through the simulation of LLM agents capable of\n",
    "interacting with and executing actions in controlled environments.\n",
    "We also analyse and visualise publicly available data in an attempt\n",
    "to create realistic but not quite exact copies of the originals. Our\n",
    "early results show some promise, with outputs that resemble the\n",
    "twin peak curves for household energy consumption. The hope is\n",
    "that our generalised approach can be used to easily replicate usable\n",
    "and realistic copies of otherwise secret or sensitive data\n",
    "\"\"\",\n",
    "9: \"\"\"\n",
    "This paper introduces a method for personaliz-\n",
    "ing energy optimization using large language models (LLMs)\n",
    "combined with an optimization solver. This approach, termed\n",
    "human-guided optimization autoformalism, translates natural\n",
    "language specifications into optimization problems, enabling\n",
    "LLMs to handle various user-specific energy-related tasks. It\n",
    "allows for nuanced understanding and nonlinear reasoning\n",
    "tailored to individual preferences. The research covers common\n",
    "energy sector tasks like electric vehicle charging, HVAC control,\n",
    "and long-term planning for renewable energy installations. This\n",
    "novel strategy represents a significant advancement in context-\n",
    "based optimization using LLMs, facilitating sustainable energy\n",
    "practices customized to individual needs\n",
    "\"\"\"\n",
    "}\n",
    "#the text for rag is used as an input to the BERT model\n",
    "\n",
    "#The tokenized inputs are passed to the BERT model for processing.\n",
    "#(#remember padding=True: Ensures that all inputs are padded to the same length, allowing batch processing.)\n",
    "#The model outputs a tensor (last_hidden_state), where each input token is represented by a high-dimensional vector.\n",
    "#last_hidden_state is of shape (batch_size, sequence_length, hidden_size), where:\n",
    "#batch_size: Number of input texts.\n",
    "#sequence_length: Length of each tokenized text (after padding).\n",
    "#hidden_size: Dimensionality of the vector representation for each token (default 768 for bert-base-uncased).\n",
    "\n",
    "#last_hidden_state[:, 0]: Selects the representation of the [CLS] token for each input text. The [CLS] token is a special token added at the start of each input and is often used as the aggregate representation for the entire sequence.\n",
    "tokenized_inputs = tokenizer(list(abstracts_dict.values()), padding=True, return_tensors=\"pt\")\n",
    "tokenized_inputs = {key: value.to(\"cuda\") for key, value in tokenized_inputs.items()}\n",
    "abstract_vectors = model(**tokenized_inputs).last_hidden_state[:, 0]\n",
    "\n",
    "#abstract_vectors is a tensor of shape (batch_size, hidden_size) (e.g., (3, 768) in this case), representing each text as a single 768-dimensional vector.\n",
    "\n",
    "print(abstract_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "With our text data vectorized and indexed, we can now perform searches. We will define a function to search the index for the most relevant documents based on a query.\n",
    "\n",
    "To perform the search, we need a function (search documents) where we perform the cosine similarity between the query vector and all the abstract vectors. This function will give our the top-k indexes. Once we find the top-k indexes, with another function, we can collect the full text of the documents from the paper dictionary.\n",
    "\n",
    "To compute cosine similarity, refer to the following formula\n",
    "\n",
    "```cs = cosine_similarity(vector_a.detach().numpy(), vector_b.detach().numpy())```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k similar indices: [[5 4 3]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_top_k_similar_indices(query_vector, abstract_vectors, k):\n",
    "    \n",
    "    #Computes the top k indices of the most similar abstracts to the query based on cosine similarity.\n",
    "    \n",
    "    #Parameters:\n",
    "    #- query_vector: A tensor of shape (1, hidden_size) representing the query vector.\n",
    "    #- abstract_vectors: A tensor of shape (batch_size, hidden_size) representing the abstract vectors.\n",
    "    #- k: The number of top indices to return.\n",
    "    \n",
    "    #Returns:\n",
    "    #- sorted_indices: A numpy array of shape (1, k) containing the indices of the top k most similar abstracts.\n",
    "    similarities = cosine_similarity(query_vector.cpu().detach().numpy(), abstract_vectors.cpu().detach().numpy())\n",
    "    sorted_indices = np.argsort(-similarities, axis=1)[:, ::-1]\n",
    "\n",
    "\n",
    "    return sorted_indices[:, :k]\n",
    "\n",
    "\n",
    "def retrieve_documents(indices, documents_dict):\n",
    "    \n",
    "    #Retrieves the documents corresponding to the given indices and concatenates them into a single string.\n",
    "    \n",
    "    #Parameters:\n",
    "    #- indices: A numpy array or list of top-k indices of the most similar documents.\n",
    "    #- documents_dict: A dictionary where keys are document indices (integers) and values are the document texts (strings).\n",
    "    \n",
    "    #Returns:\n",
    "    #- concatenated_documents: A string containing the concatenated texts of the retrieved documents.\n",
    "    retrieve_docs = [documents_dict[idx] for idx in indices[0]]\n",
    "    concatenated_documents = \" \".join(retrieve_docs)\n",
    "    return concatenated_documents\n",
    "\n",
    "\n",
    "\n",
    "#now I create a vector also for my query \n",
    "\n",
    "query = \"energy topics covered by papers with large language models\"\n",
    "\n",
    "tokenized_query = tokenizer(query, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "query_vector = model(**tokenized_query).last_hidden_state[:, 0]\n",
    "\n",
    "indices = get_top_k_similar_indices(query_vector, abstract_vectors, k=3)\n",
    "print(\"Top-k similar indices:\", indices)\n",
    "\n",
    "concatenated_docs = retrieve_documents(indices, abstracts_dict)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to perform Retrieval Augmented Generation\n",
    "\n",
    "In this step, we’ll combine the context retrieved from our documents with LLAMA to generate responses. The context will provide the necessary information to the model to produce more accurate and relevant answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e781c8edcd0401db06a470b3214fabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 88.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mmeta-llama/Llama-3.2-3B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#now we put it all together\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dorot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:770\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    767\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    768\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     _load_parameter_into_model(model, param_name, \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# TODO naming is stupid it loads it as well\u001b[39;00m\n\u001b[32m    774\u001b[39m     hf_quantizer.create_quantized_param(model, param, param_name, param_device)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 88.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#now we put it all together\n",
    "\n",
    "def generate_augmented_response(query, documents):\n",
    "\n",
    "    system = \"\"\"<<SYS>> You are a helpful, respectful and honest assistant. \\\n",
    "    Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, \\\n",
    "    unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses \\\n",
    "    are socially unbiased and positive in nature. If a question does not make any sense, or is not factually \\\n",
    "    coherent, explain why instead of answering something not correct. If you don't know the answer to a question, \\\n",
    "    please don't share false information.\n",
    "    Provide straight to the point answers, without bullet points.\n",
    "<</SYS>>\"\"\"            #TODO: define system prompt\n",
    "\n",
    "    context = \" \".join(documents)              #TODO: concatenate here all the search results\n",
    "\n",
    "    prompt = \"\"\"{SYS}\\nQuestion: {PROMPT} Collect information from the following article: {BODY}. \\nAnswer:\"\"\".format(SYS=system, BODY=context, PROMPT=query)\n",
    "\n",
    "    #perform a query with LLAMA in the usual way\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    output = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=500,\n",
    "        max_new_tokens=300,  # Limit output to a reasonable size (adjust as needed)\n",
    "        top_k=10,  # Sampling parameters\n",
    "        num_return_sequences=1,  # Generate a single response\n",
    "        temperature=0.7,  # Adjust randomness\n",
    "        repetition_penalty=1.0,  # Reduce repetition\n",
    "        length_penalty=1.0,  # Penalize longer responses\n",
    "        eos_token_id=tokenizer.eos_token_id,  # End generation at EOS token\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Avoid padding tokens\n",
    "        early_stopping=True  # Stop once the model generates the response   \n",
    "    )\n",
    "    #return the response\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# TODO: generate the queries!\n",
    "query = \"What are the main energy issues for Large Language Models?\"\n",
    "\n",
    "response = generate_augmented_response(query, concatenated_docs)\n",
    "print(response)\n",
    "\n",
    "#TODO: now compare the results with a prompt without RAG. What are the results?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
